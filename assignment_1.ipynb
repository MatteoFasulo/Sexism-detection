{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/MatteoFasulo/Sexism-detection/blob/main/assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownloader\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgloader\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "import re\n",
    "import json\n",
    "from typing import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import umap\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import gensim\n",
    "import gensim.downloader as gloader\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SexismDetector:\n",
    "    def __init__(self):\n",
    "        \n",
    "        URL_PATTERN_STR = r\"\"\"(?i)((?:https?:(?:/{1,3}|[a-z0-9%])|[a-z0-9.\\-]+[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info\n",
    "                      |int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|\n",
    "                      bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|\n",
    "                      cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|\n",
    "                      gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|\n",
    "                      la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|\n",
    "                      nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|\n",
    "                      sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|\n",
    "                      uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)/)(?:[^\\s()<>{}\\[\\]]+|\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]\n",
    "                      *?\\)|\\([^\\s]+?\\))+(?:\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’])|(?:(?<!@)\n",
    "                      [a-z0-9]+(?:[.\\-][a-z0-9]+)*[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name\n",
    "                      |post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn\n",
    "                      |bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg\n",
    "                      |eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id\n",
    "                      |ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|\n",
    "                      md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|\n",
    "                      ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|\n",
    "                      sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|\n",
    "                      za|zm|zw)\\b/?(?!@)))\"\"\"\n",
    "        self.URL_PATTERN = re.compile(URL_PATTERN_STR, re.IGNORECASE)\n",
    "        self.HASHTAG_PATTERN = re.compile(r'#\\w*')\n",
    "        self.MENTION_PATTERN = re.compile(r'@\\w*')\n",
    "        self.EMOJIS_PATTERN = re.compile(u'([\\U00002600-\\U000027BF])|([\\U0001f300-\\U0001f64F])|([\\U0001f680-\\U0001f6FF])')\n",
    "        self.SPECIAL_CHARACTERS_PATTERN = re.compile(r'&lt;/?[a-z]+&gt;')\n",
    "        self.AND_PATTERN = re.compile(r'&amp;')\n",
    "        self.WORD_PATTERN = re.compile(r'[^a-zA-Z\\s]')\n",
    "        self.SEED = 42\n",
    "        self.DATA_FOLDER = Path('data')\n",
    "        self.columns_to_maintain = ['id_EXIST', 'lang', 'tweet', 'hard_label_task1']\n",
    "        self.UNK_TOKEN = '[UNK]'\n",
    "        self.PAD_TOKEN = '[PAD]'\n",
    "\n",
    "    def download_corpus(self, url: str, filename: str):\n",
    "        if not self.DATA_FOLDER.exists():\n",
    "            self.DATA_FOLDER.mkdir(parents=True)\n",
    "            print(f\"Created folder {self.DATA_FOLDER}.\")\n",
    "            \n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        with open(self.DATA_FOLDER / filename, 'w', encoding='utf-8') as f:\n",
    "            f.write(response.text)\n",
    "\n",
    "    def load_corpus(self, filename: str, *args, **kwargs):\n",
    "        return pd.read_json(self.DATA_FOLDER / filename, *args, **kwargs)\n",
    "\n",
    "    @staticmethod\n",
    "    def majority_voting(votes: list[str]) -> str:\n",
    "        total_num_votes = len(votes)\n",
    "        yes_votes = votes.count(\"YES\")\n",
    "        no_votes = total_num_votes - yes_votes\n",
    "\n",
    "        if yes_votes > no_votes:\n",
    "            return \"YES\"\n",
    "        elif no_votes > yes_votes:\n",
    "            return \"NO\"\n",
    "        else:\n",
    "            return \"NEUTRAL\" # This will be the case when there is a tie (removed later)\n",
    "\n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        text = self.URL_PATTERN.sub('', text)\n",
    "        text = self.MENTION_PATTERN.sub('', text)\n",
    "        text = self.HASHTAG_PATTERN.sub('', text)\n",
    "        text = self.EMOJIS_PATTERN.sub('', text)\n",
    "        text = self.SPECIAL_CHARACTERS_PATTERN.sub('', text)\n",
    "        text = self.AND_PATTERN.sub('and', text)\n",
    "        text = text.strip()\n",
    "        text = self.WORD_PATTERN.sub(' ', text)\n",
    "        text = text.strip()\n",
    "        return text\n",
    "\n",
    "    def lemmatize_text(self, text: str) -> str:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        downloaded = False\n",
    "        while not downloaded:\n",
    "            try:\n",
    "                lemmatizer.lemmatize(text)\n",
    "                downloaded = True\n",
    "            except LookupError:\n",
    "                print(\"Downloading WordNet...\")\n",
    "                nltk.download('wordnet')\n",
    "        return ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "    @staticmethod\n",
    "    def text_diff(original_text: str, preprocessed_text: str, random: bool = True):\n",
    "        if random:\n",
    "            idx = np.random.randint(0, preprocessed_text.shape[0])\n",
    "        else:\n",
    "            idx = 0\n",
    "\n",
    "        print(f\"Original tweet:\\n{original_text['tweet'].iloc[idx]}\")\n",
    "        print(f\"Processed tweet:\\n{preprocessed_text['tweet'].iloc[idx]}\")\n",
    "\n",
    "    def load_glove(self, model_name: str = 'glove-wiki-gigaword', embedding_dim: int = 50):\n",
    "        self.EMBEDDING_DIM = embedding_dim\n",
    "        return gloader.load(f\"{model_name}-{embedding_dim}\")\n",
    "\n",
    "    def get_vocab(self, data: pd.DataFrame, word_listing: list = None) -> OrderedDict:\n",
    "        idx_to_word = OrderedDict()\n",
    "        word_to_idx = OrderedDict()\n",
    "\n",
    "        tokenizer = nltk.tokenize.NLTKWordTokenizer()\n",
    "\n",
    "        if word_listing is None:\n",
    "            curr_idx = 0\n",
    "            for sentence in data['tweet'].values:\n",
    "                tokens = tokenizer.tokenize(sentence)\n",
    "                for token in tokens:\n",
    "                    if token not in word_to_idx:\n",
    "                        word_to_idx[token] = curr_idx\n",
    "                        idx_to_word[curr_idx] = token\n",
    "                        curr_idx += 1\n",
    "\n",
    "        else:\n",
    "            word_to_idx[self.UNK_TOKEN] = 0\n",
    "            idx_to_word[0] = self.UNK_TOKEN\n",
    "\n",
    "            curr_idx = 1\n",
    "            for sentence in data['tweet'].values:\n",
    "                tokens = sentence.split()\n",
    "                for token in word_listing:\n",
    "                    if token not in word_to_idx:\n",
    "                        word_to_idx[token] = curr_idx\n",
    "                        idx_to_word[curr_idx] = token\n",
    "                        curr_idx += 1\n",
    "        \n",
    "        return idx_to_word, word_to_idx\n",
    "\n",
    "    def get_augmented_vocab(self, emb_model: gensim.models.keyedvectors.KeyedVectors, train_words: list, save: bool = False) -> gensim.models.keyedvectors.KeyedVectors:\n",
    "        embedding_vocab = set(emb_model.key_to_index.keys())\n",
    "\n",
    "        new_tokens = []\n",
    "        new_vectors = []\n",
    "        for token in train_words:\n",
    "            if token not in embedding_vocab:\n",
    "                embedding_vocab.add(token)\n",
    "            try:\n",
    "                embedding_vec = emb_model.get_vector(token)\n",
    "            except (KeyError, ValueError):\n",
    "                embedding_vec = np.random.uniform(low=-0.05, high=0.05, size=self.EMBEDDING_DIM)\n",
    "\n",
    "            new_tokens.append(token)\n",
    "            new_vectors.append(embedding_vec)\n",
    "\n",
    "        emb_model.add_vectors(new_tokens, new_vectors)\n",
    "\n",
    "        if save:\n",
    "            vocab_path = self.DATA_FOLDER / 'vocab2.json'\n",
    "            print(f\"Saving vocab to {vocab_path}\")\n",
    "            with vocab_path.open('w', encoding='utf-8') as f:\n",
    "                json.dump(emb_model.key_to_index, f, indent=4)\n",
    "            print(\"Vocab saved!\")\n",
    "\n",
    "        return emb_model\n",
    "\n",
    "    def get_oov_stats(self, embedding_model: gensim.models.keyedvectors.KeyedVectors, word_listing: list) -> None:\n",
    "        OOV_count = set(word_listing).difference(set(embedding_model.key_to_index.keys()))\n",
    "        OOV_percentage = float(len(OOV_count)) * 100 / len(word_listing)\n",
    "\n",
    "        print(f\"Total OOV terms: {len(OOV_count)} ({OOV_percentage:.2f}%)\")\n",
    "\n",
    "    @staticmethod\n",
    "    def visualize_embeddings(embeddings: np.ndarray,\n",
    "                         word_annotations,\n",
    "                         word_to_idx):\n",
    "        \"\"\"\n",
    "        Plots given reduce word embeddings (2D). Users can highlight specific words (word_annotations list).\n",
    "\n",
    "        :param embeddings: word embedding matrix of shape (words, 2) retrieved via a dimensionality reduction technique.\n",
    "        :param word_annotations: list of words to be annotated.\n",
    "        :param word_to_idx: vocabulary map (word -> index) (dict)\n",
    "        \"\"\"\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(15, 12))\n",
    "\n",
    "        if word_annotations:\n",
    "            print(f\"Annotating words: {word_annotations}\")\n",
    "\n",
    "            word_indexes = []\n",
    "            for word in word_annotations:\n",
    "                word_index = word_to_idx[word]\n",
    "                word_indexes.append(word_index)\n",
    "\n",
    "            word_indexes = np.array(word_indexes)\n",
    "\n",
    "            other_embeddings = embeddings[np.setdiff1d(np.arange(embeddings.shape[0]), word_indexes)]\n",
    "            target_embeddings = embeddings[word_indexes]\n",
    "\n",
    "            ax.scatter(other_embeddings[:, 0], other_embeddings[:, 1], alpha=0.1, c='blue')\n",
    "            ax.scatter(target_embeddings[:, 0], target_embeddings[:, 1], alpha=1.0, c='red')\n",
    "            ax.scatter(target_embeddings[:, 0], target_embeddings[:, 1], alpha=1, facecolors='none', edgecolors='r', s=1000)\n",
    "\n",
    "            for word, word_index in zip(word_annotations, word_indexes):\n",
    "                word_x, word_y = embeddings[word_index, 0], embeddings[word_index, 1]\n",
    "                ax.annotate(word, xy=(word_x, word_y))\n",
    "        else:\n",
    "            ax.scatter(embeddings[:, 0], embeddings[:, 1], alpha=0.1, c='blue')\n",
    "\n",
    "        # We avoid outliers ruining the visualization if they are quite far away\n",
    "        axis_x_limit = (np.min(embeddings[:, 0]), np.max(embeddings[:, 0]))\n",
    "        axis_y_limit = (np.min(embeddings[:, 1]), np.max(embeddings[:, 1]))\n",
    "        plt.xlim(left=axis_x_limit[0] - 0.5, right=axis_x_limit[1] + 0.5)\n",
    "        plt.ylim(bottom=axis_y_limit[0] - 0.5, top=axis_y_limit[1] + 0.5)\n",
    "        ax.set_xlim(axis_x_limit[0], axis_x_limit[1])\n",
    "        ax.set_ylim(axis_y_limit[0], axis_y_limit[1])\n",
    "\n",
    "    @staticmethod\n",
    "    def reduce_SVD(embeddings: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Applies SVD dimensionality reduction.\n",
    "\n",
    "        :param embeddings: word embedding matrix of shape (words, dim). In the case\n",
    "                        of a word-word co-occurrence matrix the matrix shape would\n",
    "                        be (words, words).\n",
    "\n",
    "        :return\n",
    "            - 2-dimensional word embedding matrix of shape (words, 2)\n",
    "        \"\"\"\n",
    "        print(\"Running SVD reduction method...\")\n",
    "        svd = TruncatedSVD(n_components=2, n_iter=10)\n",
    "        reduced = svd.fit_transform(embeddings)\n",
    "        print(\"SVD reduction completed!\")\n",
    "\n",
    "        return reduced\n",
    "\n",
    "    @staticmethod\n",
    "    def reduce_tSNE(embeddings: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Applies t-SNE dimensionality reduction.\n",
    "        \"\"\"\n",
    "        print(\"Running t-SNE reduction method... (it may take a while...)\")\n",
    "        tsne = TSNE(n_components=2, n_iter=1000, metric='cosine')\n",
    "        reduced = tsne.fit_transform(embeddings)\n",
    "        print(\"t-SNE reduction completed!\")\n",
    "\n",
    "        return reduced\n",
    "\n",
    "    @staticmethod\n",
    "    def reduce_umap(embeddings: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Applies UMAP dimensionality reduction.\n",
    "        \"\"\"\n",
    "        print(\"Running UMAP reduction method... (it may take a while...)\")\n",
    "        umap_emb = umap.UMAP(n_components=2, metric='cosine')\n",
    "        reduced = umap_emb.fit_transform(embeddings)\n",
    "        print(\"UMAP reduction completed!\")\n",
    "        \n",
    "        return reduced\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Corpus\n",
    "\n",
    "1. Download the data\n",
    "2. Load the JSON files and encode them as a DataFrame\n",
    "3. Generate hard labels for Task 1 with majority voting\n",
    "4. Filter the DataFrame for only english tweets\n",
    "5. Remove unwanted columns\n",
    "6. Encode the hard labels column as integers\n",
    "\n",
    ">**Bonus**: explore also Spanish tweets leveraging multi-language models and assessing the performance of the model on the two languages in comparison to the English-only model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = SexismDetector()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector.download_corpus(url='https://raw.githubusercontent.com/nlp-unibo/nlp-course-material/refs/heads/main/2024-2025/Assignment%201/data/training.json', filename='training.json')\n",
    "detector.download_corpus(url='https://raw.githubusercontent.com/nlp-unibo/nlp-course-material/refs/heads/main/2024-2025/Assignment%201/data/test.json', filename='test.json')\n",
    "detector.download_corpus(url='https://raw.githubusercontent.com/nlp-unibo/nlp-course-material/refs/heads/main/2024-2025/Assignment%201/data/validation.json', filename='validation.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the JSON files and encode them as a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = detector.load_corpus('training.json', orient='index', encoding='utf-8')\n",
    "test = detector.load_corpus('test.json', orient='index', encoding='utf-8')\n",
    "val = detector.load_corpus('validation.json', orient='index', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_EXIST</th>\n",
       "      <th>lang</th>\n",
       "      <th>tweet</th>\n",
       "      <th>number_annotators</th>\n",
       "      <th>annotators</th>\n",
       "      <th>gender_annotators</th>\n",
       "      <th>age_annotators</th>\n",
       "      <th>labels_task1</th>\n",
       "      <th>labels_task2</th>\n",
       "      <th>labels_task3</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100001</th>\n",
       "      <td>100001</td>\n",
       "      <td>es</td>\n",
       "      <td>@TheChiflis Ignora al otro, es un capullo.El p...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_1, Annotator_2, Annotator_3, Annota...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 46+, 23-45, 18-22]</td>\n",
       "      <td>[YES, YES, NO, YES, YES, YES]</td>\n",
       "      <td>[REPORTED, JUDGEMENTAL, -, REPORTED, JUDGEMENT...</td>\n",
       "      <td>[[OBJECTIFICATION], [OBJECTIFICATION, SEXUAL-V...</td>\n",
       "      <td>TRAIN_ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100002</th>\n",
       "      <td>100002</td>\n",
       "      <td>es</td>\n",
       "      <td>@ultimonomada_ Si comicsgate se parece en algo...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_7, Annotator_8, Annotator_9, Annota...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 46+, 23-45, 18-22]</td>\n",
       "      <td>[NO, NO, NO, NO, YES, NO]</td>\n",
       "      <td>[-, -, -, -, DIRECT, -]</td>\n",
       "      <td>[[-], [-], [-], [-], [OBJECTIFICATION], [-]]</td>\n",
       "      <td>TRAIN_ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100003</th>\n",
       "      <td>100003</td>\n",
       "      <td>es</td>\n",
       "      <td>@Steven2897 Lee sobre Gamergate, y como eso ha...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_7, Annotator_8, Annotator_9, Annota...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 46+, 23-45, 18-22]</td>\n",
       "      <td>[NO, NO, NO, NO, NO, NO]</td>\n",
       "      <td>[-, -, -, -, -, -]</td>\n",
       "      <td>[[-], [-], [-], [-], [-], [-]]</td>\n",
       "      <td>TRAIN_ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100004</th>\n",
       "      <td>100004</td>\n",
       "      <td>es</td>\n",
       "      <td>@Lunariita7 Un retraso social bastante lamenta...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_13, Annotator_14, Annotator_15, Ann...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 46+, 23-45, 18-22]</td>\n",
       "      <td>[NO, NO, YES, NO, YES, YES]</td>\n",
       "      <td>[-, -, DIRECT, -, REPORTED, REPORTED]</td>\n",
       "      <td>[[-], [-], [IDEOLOGICAL-INEQUALITY], [-], [IDE...</td>\n",
       "      <td>TRAIN_ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100005</th>\n",
       "      <td>100005</td>\n",
       "      <td>es</td>\n",
       "      <td>@novadragon21 @icep4ck @TvDannyZ Entonces como...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_19, Annotator_20, Annotator_21, Ann...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 46+, 23-45, 18-22]</td>\n",
       "      <td>[YES, NO, YES, NO, YES, YES]</td>\n",
       "      <td>[REPORTED, -, JUDGEMENTAL, -, JUDGEMENTAL, DIR...</td>\n",
       "      <td>[[STEREOTYPING-DOMINANCE, OBJECTIFICATION], [-...</td>\n",
       "      <td>TRAIN_ES</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id_EXIST lang                                              tweet  \\\n",
       "100001    100001   es  @TheChiflis Ignora al otro, es un capullo.El p...   \n",
       "100002    100002   es  @ultimonomada_ Si comicsgate se parece en algo...   \n",
       "100003    100003   es  @Steven2897 Lee sobre Gamergate, y como eso ha...   \n",
       "100004    100004   es  @Lunariita7 Un retraso social bastante lamenta...   \n",
       "100005    100005   es  @novadragon21 @icep4ck @TvDannyZ Entonces como...   \n",
       "\n",
       "        number_annotators                                         annotators  \\\n",
       "100001                  6  [Annotator_1, Annotator_2, Annotator_3, Annota...   \n",
       "100002                  6  [Annotator_7, Annotator_8, Annotator_9, Annota...   \n",
       "100003                  6  [Annotator_7, Annotator_8, Annotator_9, Annota...   \n",
       "100004                  6  [Annotator_13, Annotator_14, Annotator_15, Ann...   \n",
       "100005                  6  [Annotator_19, Annotator_20, Annotator_21, Ann...   \n",
       "\n",
       "         gender_annotators                          age_annotators  \\\n",
       "100001  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
       "100002  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
       "100003  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
       "100004  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
       "100005  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
       "\n",
       "                         labels_task1  \\\n",
       "100001  [YES, YES, NO, YES, YES, YES]   \n",
       "100002      [NO, NO, NO, NO, YES, NO]   \n",
       "100003       [NO, NO, NO, NO, NO, NO]   \n",
       "100004    [NO, NO, YES, NO, YES, YES]   \n",
       "100005   [YES, NO, YES, NO, YES, YES]   \n",
       "\n",
       "                                             labels_task2  \\\n",
       "100001  [REPORTED, JUDGEMENTAL, -, REPORTED, JUDGEMENT...   \n",
       "100002                            [-, -, -, -, DIRECT, -]   \n",
       "100003                                 [-, -, -, -, -, -]   \n",
       "100004              [-, -, DIRECT, -, REPORTED, REPORTED]   \n",
       "100005  [REPORTED, -, JUDGEMENTAL, -, JUDGEMENTAL, DIR...   \n",
       "\n",
       "                                             labels_task3     split  \n",
       "100001  [[OBJECTIFICATION], [OBJECTIFICATION, SEXUAL-V...  TRAIN_ES  \n",
       "100002       [[-], [-], [-], [-], [OBJECTIFICATION], [-]]  TRAIN_ES  \n",
       "100003                     [[-], [-], [-], [-], [-], [-]]  TRAIN_ES  \n",
       "100004  [[-], [-], [IDEOLOGICAL-INEQUALITY], [-], [IDE...  TRAIN_ES  \n",
       "100005  [[STEREOTYPING-DOMINANCE, OBJECTIFICATION], [-...  TRAIN_ES  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6920, 11), (726, 11), (312, 11))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, val.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate hard labels for Task 1 with majority voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['hard_label_task1'] = train['labels_task1'].apply(detector.majority_voting)\n",
    "val['hard_label_task1'] = val['labels_task1'].apply(detector.majority_voting)\n",
    "test['hard_label_task1'] = test['labels_task1'].apply(detector.majority_voting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter the DataFrame for only english tweets and remove unclear tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[(train['hard_label_task1'] != \"NEUTRAL\") & (train['lang'] == \"en\")]\n",
    "val = val[(val['hard_label_task1'] != \"NEUTRAL\") & (val['lang'] == \"en\")]\n",
    "test = test[(test['hard_label_task1'] != \"NEUTRAL\") & (test['lang'] == \"en\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2870, 12), (158, 12), (286, 12))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, val.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove unwanted columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[detector.columns_to_maintain]\n",
    "val = val[detector.columns_to_maintain]\n",
    "test = test[detector.columns_to_maintain]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_EXIST</th>\n",
       "      <th>lang</th>\n",
       "      <th>tweet</th>\n",
       "      <th>hard_label_task1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200002</th>\n",
       "      <td>200002</td>\n",
       "      <td>en</td>\n",
       "      <td>Writing a uni essay in my local pub with a cof...</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200003</th>\n",
       "      <td>200003</td>\n",
       "      <td>en</td>\n",
       "      <td>@UniversalORL it is 2021 not 1921. I dont appr...</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200006</th>\n",
       "      <td>200006</td>\n",
       "      <td>en</td>\n",
       "      <td>According to a customer I have plenty of time ...</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200007</th>\n",
       "      <td>200007</td>\n",
       "      <td>en</td>\n",
       "      <td>So only 'blokes' drink beer? Sorry, but if you...</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200008</th>\n",
       "      <td>200008</td>\n",
       "      <td>en</td>\n",
       "      <td>New to the shelves this week - looking forward...</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id_EXIST lang                                              tweet  \\\n",
       "200002    200002   en  Writing a uni essay in my local pub with a cof...   \n",
       "200003    200003   en  @UniversalORL it is 2021 not 1921. I dont appr...   \n",
       "200006    200006   en  According to a customer I have plenty of time ...   \n",
       "200007    200007   en  So only 'blokes' drink beer? Sorry, but if you...   \n",
       "200008    200008   en  New to the shelves this week - looking forward...   \n",
       "\n",
       "       hard_label_task1  \n",
       "200002              YES  \n",
       "200003              YES  \n",
       "200006              YES  \n",
       "200007              YES  \n",
       "200008               NO  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode the hard labels column as integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['hard_label_task1'] = train['hard_label_task1'].apply(lambda x: 1 if x == \"YES\" else 0)\n",
    "val['hard_label_task1'] = val['hard_label_task1'].apply(lambda x: 1 if x == \"YES\" else 0)\n",
    "test['hard_label_task1'] = test['hard_label_task1'].apply(lambda x: 1 if x == \"YES\" else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hard_label_task1\n",
       "0    1733\n",
       "1    1137\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.hard_label_task1.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Data Cleaning\n",
    "\n",
    "1. Remove emojis\n",
    "2. Remove hashtags (e.g. #metoo)\n",
    "3. Remove mentions (e.g. @user)\n",
    "4. Remove URLs\n",
    "5. Remove special characters and symbols\n",
    "6. Remove specific quote characters (e.g. curly quotes)\n",
    "7. Perform lemmatization\n",
    "\n",
    ">**Bonus**: use other preprocessing strategies exploring techniques tailored specifically for tweets or methods that are common in social media text processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way to go (priority order) is the following:\n",
    "1. Remove URLs\n",
    "2. Remove mentions\n",
    "3. Remove hashtags\n",
    "4. Remove emojis\n",
    "5. Remove special characters\n",
    "6. Remove specific quote characters\n",
    "7. Perform lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_train = train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['tweet'] = train['tweet'].apply(detector.preprocess_text)\n",
    "val['tweet'] = val['tweet'].apply(detector.preprocess_text)\n",
    "test['tweet'] = test['tweet'].apply(detector.preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['tweet'] = train['tweet'].apply(detector.lemmatize_text)\n",
    "val['tweet'] = val['tweet'].apply(detector.lemmatize_text)\n",
    "test['tweet'] = test['tweet'].apply(detector.lemmatize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cast text to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['tweet'] = train['tweet'].str.lower()\n",
    "val['tweet'] = val['tweet'].str.lower()\n",
    "test['tweet'] = test['tweet'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the difference between the original and cleaned text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tweet:\n",
      "@bobbybenitez81 In what sense? Elders wives are mostly busybodies who're constantly meddling in peoples business but women have no clout to infiltrate the boys club. A woman's silence is having her rights removed as it relates to taking the lead and speaking in a teaching capacity in the KHs\n",
      "Processed tweet:\n",
      "in what sense elders wife are mostly busybody who re constantly meddling in people business but woman have no clout to infiltrate the boy club a woman s silence is having her right removed a it relates to taking the lead and speaking in a teaching capacity in the khs\n"
     ]
    }
   ],
   "source": [
    "detector.text_diff(preprocessed_text=train, original_text=original_train, random=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Text Encoding\n",
    "\n",
    "* Embed words using GloVe embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embed words using GloVe embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_model = detector.load_glove(model_name='glove-wiki-gigaword', embedding_dim=50)\n",
    "len(emb_model.key_to_index.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx_to_word, train_word_to_idx = detector.get_vocab(train)\n",
    "train_word_listing = list(train_idx_to_word.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total OOV terms: 868 (8.81%)\n"
     ]
    }
   ],
   "source": [
    "detector.get_oov_stats(emb_model, train_word_listing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving vocab to data\\vocab2.json\n",
      "Vocab saved!\n"
     ]
    }
   ],
   "source": [
    "emb_model_augmented = detector.get_augmented_vocab(emb_model, train_words=train_word_listing, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_idx_to_word, val_word_to_idx = detector.get_vocab(val, word_listing=train_word_listing)\n",
    "test_idx_to_word, test_word_to_idx = detector.get_vocab(test, word_listing=train_word_listing)\n",
    "\n",
    "val_word_listing = list(val_idx_to_word.values())\n",
    "test_word_listing = list(test_idx_to_word.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the UNK token to the embedding model with the vector which is the average of all the vectors\n",
    "emb_model_augmented.add_vectors([\"[UNK]\", \"[PAD]\"], [np.mean(emb_model.vectors, axis=0), np.zeros(50)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total OOV terms: 0 (0.00%)\n",
      "Total OOV terms: 0 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "detector.get_oov_stats(emb_model_augmented, val_word_listing)\n",
    "detector.get_oov_stats(emb_model_augmented, test_word_listing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400870, 50)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Create the embeddings matrix\n",
    "embedding_matrix = emb_model.vectors\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP\n",
    "#reduced_embedding_umap = detector.reduce_umap(embedding_matrix)\n",
    "#detector.visualize_embeddings(reduced_embedding_umap, ['whore', 'woman', 'slut', 'girl', 'man', 'boy'], emb_model.key_to_index)\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Model definition\n",
    "\n",
    "* Baseline: Implement a Bidirectional LSTM with a Dense layer on top.\n",
    "* Model 1: add an additional LSTM layer to the baseline model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline: Implement a Bidirectional LSTM with a Dense layer on top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(sequences: list, embedding_model):\n",
    "    # instantiate the tokenizer\n",
    "    tokenizer = nltk.NLTKWordTokenizer()\n",
    "    # tokenize each sequence\n",
    "    tokenized_seqs = [tokenizer.tokenize(seq) for seq in sequences]\n",
    "    # convert them to numerical form\n",
    "    numerical_seqs = ([torch.tensor([embedding_model.get_index(token) for token in seq]) for seq in tokenized_seqs])\n",
    "    # pad the sequences and cast them to tensor\n",
    "    return torch.nn.utils.rnn.pad_sequence(sequences=numerical_seqs, batch_first=True, padding_value=embedding_model.get_index(detector.PAD_TOKEN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  1649,      7,  36352,  ..., 400869, 400869, 400869],\n",
       "        [    20,     14,     36,  ..., 400869, 400869, 400869],\n",
       "        [   200,      4,      7,  ..., 400869, 400869, 400869],\n",
       "        ...,\n",
       "        [   190,    285,    411,  ..., 400869, 400869, 400869],\n",
       "        [   738,     88,     81,  ..., 400869, 400869, 400869],\n",
       "        [ 23088,     61,  27487,  ..., 400869, 400869, 400869]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepare_sequence(sequences=train['tweet'], embedding_model=emb_model_augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data: pd.DataFrame, embedding_model):\n",
    "        self.data = data\n",
    "        self.embedding_model = embedding_model\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        tweet_text = self.data['tweet'].iloc[index]\n",
    "        encoded_tweet = prepare_sequence(sequences=[tweet_text], embedding_model=self.embedding_model)\n",
    "        return encoded_tweet, self.data['hard_label_task1'].iloc[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = CustomDataset(data=train, embedding_model=emb_model_augmented)\n",
    "train_dataloader = torch.utils.data.DataLoader(data, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 50]), torch.Size([1]))"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataloader))[0].shape, next(iter(train_dataloader))[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataloader = torch.utils.data.DataLoader(prepare_sequence(sequences=train['tweet'], embedding_model=emb_model_augmented), batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[120], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_function(tag_scores\u001b[38;5;241m.\u001b[39msqueeze(), labels\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39msqueeze())\n\u001b[0;32m     28\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 29\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mf:\\VSCODE\\Python\\Sexism-detection\\.venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    485\u001b[0m             )\n\u001b[1;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mf:\\VSCODE\\Python\\Sexism-detection\\.venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mf:\\VSCODE\\Python\\Sexism-detection\\.venv\\Lib\\site-packages\\torch\\optim\\adamw.py:220\u001b[0m, in \u001b[0;36mAdamW.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    207\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m cast(Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m], group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    209\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    210\u001b[0m         group,\n\u001b[0;32m    211\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    217\u001b[0m         state_steps,\n\u001b[0;32m    218\u001b[0m     )\n\u001b[1;32m--> 220\u001b[0m     \u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mf:\\VSCODE\\Python\\Sexism-detection\\.venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\VSCODE\\Python\\Sexism-detection\\.venv\\Lib\\site-packages\\torch\\optim\\adamw.py:782\u001b[0m, in \u001b[0;36madamw\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    780\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamw\n\u001b[1;32m--> 782\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\VSCODE\\Python\\Sexism-detection\\.venv\\Lib\\site-packages\\torch\\optim\\adamw.py:427\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001b[0m\n\u001b[0;32m    425\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    426\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 427\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    429\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[0;32m    431\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class BaselineLSTM(torch.nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, output_dim):\n",
    "        super(BaselineLSTM, self).__init__()\n",
    "\n",
    "        self.word_embeddings = torch.nn.Embedding.from_pretrained(torch.from_numpy(embedding_matrix), freeze=False)\n",
    "        self.lstm = torch.nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
    "        self.hidden2tag = torch.nn.Linear(hidden_dim *2, output_dim)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        tag_space = self.hidden2tag(lstm_out)\n",
    "        tag_scores = torch.sigmoid(tag_space)\n",
    "        return tag_scores\n",
    "\n",
    "model = BaselineLSTM(embedding_dim=50, hidden_dim=256, output_dim=1)\n",
    "loss_function = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "for epoch in range(20):\n",
    "    for batch in train_dataloader:\n",
    "        sentences, labels = batch\n",
    "\n",
    "        model.zero_grad()\n",
    "        tag_scores = model(sentences.squeeze(1))\n",
    "        tag_scores = tag_scores.mean(dim=1)  # Take the mean along the sequence dimension\n",
    "        loss = loss_function(tag_scores.squeeze(), labels.float().squeeze())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5: Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 6: Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 7: Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 8: Report"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
