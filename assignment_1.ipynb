{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/MatteoFasulo/Sexism-detection/blob/main/assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\VSCODE\\Python\\Sexism-detection\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "import re\n",
    "import json\n",
    "from typing import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import umap\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import gensim\n",
    "import gensim.downloader as gloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SexismDetector:\n",
    "    def __init__(self, EMBEDDING_DIM: int = 50):\n",
    "        \n",
    "        URL_PATTERN_STR = r\"\"\"(?i)((?:https?:(?:/{1,3}|[a-z0-9%])|[a-z0-9.\\-]+[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info\n",
    "                      |int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|\n",
    "                      bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|\n",
    "                      cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|\n",
    "                      gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|\n",
    "                      la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|\n",
    "                      nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|\n",
    "                      sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|\n",
    "                      uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)/)(?:[^\\s()<>{}\\[\\]]+|\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]\n",
    "                      *?\\)|\\([^\\s]+?\\))+(?:\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’])|(?:(?<!@)\n",
    "                      [a-z0-9]+(?:[.\\-][a-z0-9]+)*[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name\n",
    "                      |post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn\n",
    "                      |bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg\n",
    "                      |eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id\n",
    "                      |ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|\n",
    "                      md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|\n",
    "                      ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|\n",
    "                      sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|\n",
    "                      za|zm|zw)\\b/?(?!@)))\"\"\"\n",
    "        self.URL_PATTERN = re.compile(URL_PATTERN_STR, re.IGNORECASE)\n",
    "        self.HASHTAG_PATTERN = re.compile(r'#\\w*')\n",
    "        self.MENTION_PATTERN = re.compile(r'@\\w*')\n",
    "        self.EMOJIS_PATTERN = re.compile(u'([\\U00002600-\\U000027BF])|([\\U0001f300-\\U0001f64F])|([\\U0001f680-\\U0001f6FF])')\n",
    "        self.SPECIAL_CHARACTERS_PATTERN = re.compile(r'&lt;/?[a-z]+&gt;')\n",
    "        self.AND_PATTERN = re.compile(r'&amp;')\n",
    "        self.WORD_PATTERN = re.compile(r'[^a-zA-Z\\s]')\n",
    "        self.SEED = 42\n",
    "        self.DATA_FOLDER = Path('data')\n",
    "        self.columns_to_maintain = ['id_EXIST', 'lang', 'tweet', 'hard_label_task1']\n",
    "        self.UNK_TOKEN = '[UNK]'\n",
    "        self.EMBEDDING_DIM = EMBEDDING_DIM\n",
    "\n",
    "    def download_corpus(self, url: str, filename: str):\n",
    "        if not self.DATA_FOLDER.exists():\n",
    "            self.DATA_FOLDER.mkdir(parents=True)\n",
    "            print(f\"Created folder {self.DATA_FOLDER}.\")\n",
    "            \n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        with open(self.DATA_FOLDER / filename, 'w', encoding='utf-8') as f:\n",
    "            f.write(response.text)\n",
    "\n",
    "    def load_corpus(self, filename: str, *args, **kwargs):\n",
    "        return pd.read_json(self.DATA_FOLDER / filename, *args, **kwargs)\n",
    "\n",
    "    @staticmethod\n",
    "    def majority_voting(votes: list[str]) -> str:\n",
    "        total_num_votes = len(votes)\n",
    "        yes_votes = votes.count(\"YES\")\n",
    "        no_votes = total_num_votes - yes_votes\n",
    "\n",
    "        if yes_votes > no_votes:\n",
    "            return \"YES\"\n",
    "        elif no_votes > yes_votes:\n",
    "            return \"NO\"\n",
    "        else:\n",
    "            return \"NEUTRAL\" # This will be the case when there is a tie (removed later)\n",
    "\n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        text = self.URL_PATTERN.sub('', text)\n",
    "        text = self.MENTION_PATTERN.sub('', text)\n",
    "        text = self.HASHTAG_PATTERN.sub('', text)\n",
    "        text = self.EMOJIS_PATTERN.sub('', text)\n",
    "        text = self.SPECIAL_CHARACTERS_PATTERN.sub('', text)\n",
    "        text = self.AND_PATTERN.sub('and', text)\n",
    "        text = text.strip()\n",
    "        text = self.WORD_PATTERN.sub(' ', text)\n",
    "        text = text.strip()\n",
    "        text = text.lower()\n",
    "        return text\n",
    "\n",
    "    def lemmatize_text(self, text: str) -> str:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        downloaded = False\n",
    "        while not downloaded:\n",
    "            try:\n",
    "                lemmatizer.lemmatize(text)\n",
    "                downloaded = True\n",
    "            except LookupError:\n",
    "                print(\"Downloading WordNet...\")\n",
    "                nltk.download('wordnet')\n",
    "        return ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "    @staticmethod\n",
    "    def text_diff(original_text: str, preprocessed_text: str, random: bool = True):\n",
    "        if random:\n",
    "            idx = np.random.randint(0, preprocessed_text.shape[0])\n",
    "        else:\n",
    "            idx = 0\n",
    "\n",
    "        print(f\"Original tweet:\\n{original_text['tweet'].iloc[idx]}\")\n",
    "        print(f\"Processed tweet:\\n{preprocessed_text['tweet'].iloc[idx]}\")\n",
    "\n",
    "    def load_glove(self, model_name: str = 'glove-twitter'):\n",
    "        return gloader.load(f\"{model_name}-{self.EMBEDDING_DIM}\")\n",
    "\n",
    "    def get_vocab(self, data: pd.DataFrame, word_listing: list = None) -> OrderedDict:\n",
    "        idx_to_word = OrderedDict()\n",
    "        word_to_idx = OrderedDict()\n",
    "\n",
    "        if word_listing is None:\n",
    "            curr_idx = 0\n",
    "            for sentence in data['tweet'].values:\n",
    "                tokens = sentence.split()\n",
    "                for token in tokens:\n",
    "                    if token not in word_to_idx:\n",
    "                        word_to_idx[token] = curr_idx\n",
    "                        idx_to_word[curr_idx] = token\n",
    "                        curr_idx += 1\n",
    "\n",
    "        else:\n",
    "            word_to_idx[self.UNK_TOKEN] = 0\n",
    "            idx_to_word[0] = self.UNK_TOKEN\n",
    "\n",
    "            curr_idx = 1\n",
    "            for sentence in data['tweet'].values:\n",
    "                tokens = sentence.split()\n",
    "                for token in word_listing:\n",
    "                    if token not in word_to_idx:\n",
    "                        word_to_idx[token] = curr_idx\n",
    "                        idx_to_word[curr_idx] = token\n",
    "                        curr_idx += 1\n",
    "        \n",
    "        return idx_to_word, word_to_idx\n",
    "\n",
    "    def get_augmented_vocab(self, emb_model: gensim.models.keyedvectors.KeyedVectors, train_words: list, save: bool = False) -> gensim.models.keyedvectors.KeyedVectors:\n",
    "        embedding_vocab = set(emb_model.key_to_index.keys())\n",
    "\n",
    "        new_tokens = []\n",
    "        new_vectors = []\n",
    "        for token in train_words:\n",
    "            if token not in embedding_vocab:\n",
    "                embedding_vocab.add(token)\n",
    "            try:\n",
    "                embedding_vec = emb_model.get_vector(token)\n",
    "            except (KeyError, ValueError):\n",
    "                embedding_vec = np.random.uniform(low=-0.05, high=0.05, size=self.EMBEDDING_DIM)\n",
    "\n",
    "            new_tokens.append(token)\n",
    "            new_vectors.append(embedding_vec)\n",
    "\n",
    "        emb_model.add_vectors(new_tokens, new_vectors)\n",
    "\n",
    "        if save:\n",
    "            vocab_path = self.DATA_FOLDER / 'vocab2.json'\n",
    "            print(f\"Saving vocab to {vocab_path}\")\n",
    "            with vocab_path.open('w', encoding='utf-8') as f:\n",
    "                json.dump(emb_model.key_to_index, f, indent=4)\n",
    "            print(\"Vocab saved!\")\n",
    "\n",
    "        return emb_model\n",
    "\n",
    "    def get_oov_stats(self, embedding_model: gensim.models.keyedvectors.KeyedVectors, word_listing: list) -> None:\n",
    "        OOV_count = set(word_listing).difference(set(embedding_model.key_to_index.keys()))\n",
    "        OOV_percentage = float(len(OOV_count)) * 100 / len(word_listing)\n",
    "\n",
    "        print(f\"Total OOV terms: {len(OOV_count)} ({OOV_percentage:.2f}%)\")\n",
    "\n",
    "    @staticmethod\n",
    "    def visualize_embeddings(embeddings: np.ndarray,\n",
    "                         word_annotations,\n",
    "                         word_to_idx):\n",
    "        \"\"\"\n",
    "        Plots given reduce word embeddings (2D). Users can highlight specific words (word_annotations list).\n",
    "\n",
    "        :param embeddings: word embedding matrix of shape (words, 2) retrieved via a dimensionality reduction technique.\n",
    "        :param word_annotations: list of words to be annotated.\n",
    "        :param word_to_idx: vocabulary map (word -> index) (dict)\n",
    "        \"\"\"\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(15, 12))\n",
    "\n",
    "        if word_annotations:\n",
    "            print(f\"Annotating words: {word_annotations}\")\n",
    "\n",
    "            word_indexes = []\n",
    "            for word in word_annotations:\n",
    "                word_index = word_to_idx[word]\n",
    "                word_indexes.append(word_index)\n",
    "\n",
    "            word_indexes = np.array(word_indexes)\n",
    "\n",
    "            other_embeddings = embeddings[np.setdiff1d(np.arange(embeddings.shape[0]), word_indexes)]\n",
    "            target_embeddings = embeddings[word_indexes]\n",
    "\n",
    "            ax.scatter(other_embeddings[:, 0], other_embeddings[:, 1], alpha=0.1, c='blue')\n",
    "            ax.scatter(target_embeddings[:, 0], target_embeddings[:, 1], alpha=1.0, c='red')\n",
    "            ax.scatter(target_embeddings[:, 0], target_embeddings[:, 1], alpha=1, facecolors='none', edgecolors='r', s=1000)\n",
    "\n",
    "            for word, word_index in zip(word_annotations, word_indexes):\n",
    "                word_x, word_y = embeddings[word_index, 0], embeddings[word_index, 1]\n",
    "                ax.annotate(word, xy=(word_x, word_y))\n",
    "        else:\n",
    "            ax.scatter(embeddings[:, 0], embeddings[:, 1], alpha=0.1, c='blue')\n",
    "\n",
    "        # We avoid outliers ruining the visualization if they are quite far away\n",
    "        axis_x_limit = (np.min(embeddings[:, 0]), np.max(embeddings[:, 0]))\n",
    "        axis_y_limit = (np.min(embeddings[:, 1]), np.max(embeddings[:, 1]))\n",
    "        plt.xlim(left=axis_x_limit[0] - 0.5, right=axis_x_limit[1] + 0.5)\n",
    "        plt.ylim(bottom=axis_y_limit[0] - 0.5, top=axis_y_limit[1] + 0.5)\n",
    "        ax.set_xlim(axis_x_limit[0], axis_x_limit[1])\n",
    "        ax.set_ylim(axis_y_limit[0], axis_y_limit[1])\n",
    "\n",
    "    @staticmethod\n",
    "    def reduce_SVD(embeddings: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Applies SVD dimensionality reduction.\n",
    "\n",
    "        :param embeddings: word embedding matrix of shape (words, dim). In the case\n",
    "                        of a word-word co-occurrence matrix the matrix shape would\n",
    "                        be (words, words).\n",
    "\n",
    "        :return\n",
    "            - 2-dimensional word embedding matrix of shape (words, 2)\n",
    "        \"\"\"\n",
    "        print(\"Running SVD reduction method...\")\n",
    "        svd = TruncatedSVD(n_components=2, n_iter=10)\n",
    "        reduced = svd.fit_transform(embeddings)\n",
    "        print(\"SVD reduction completed!\")\n",
    "\n",
    "        return reduced\n",
    "\n",
    "    @staticmethod\n",
    "    def reduce_tSNE(embeddings: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Applies t-SNE dimensionality reduction.\n",
    "        \"\"\"\n",
    "        print(\"Running t-SNE reduction method... (it may take a while...)\")\n",
    "        tsne = TSNE(n_components=2, n_iter=1000, metric='cosine')\n",
    "        reduced = tsne.fit_transform(embeddings)\n",
    "        print(\"t-SNE reduction completed!\")\n",
    "\n",
    "        return reduced\n",
    "\n",
    "    @staticmethod\n",
    "    def reduce_umap(embeddings: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Applies UMAP dimensionality reduction.\n",
    "        \"\"\"\n",
    "        print(\"Running UMAP reduction method... (it may take a while...)\")\n",
    "        umap_emb = umap.UMAP(n_components=2, metric='cosine')\n",
    "        reduced = umap_emb.fit_transform(embeddings)\n",
    "        print(\"UMAP reduction completed!\")\n",
    "        \n",
    "        return reduced\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Corpus\n",
    "\n",
    "1. Download the data\n",
    "2. Load the JSON files and encode them as a DataFrame\n",
    "3. Generate hard labels for Task 1 with majority voting\n",
    "4. Filter the DataFrame for only english tweets\n",
    "5. Remove unwanted columns\n",
    "6. Encode the hard labels column as integers\n",
    "\n",
    ">**Bonus**: explore also Spanish tweets leveraging multi-language models and assessing the performance of the model on the two languages in comparison to the English-only model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = SexismDetector(EMBEDDING_DIM=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector.download_corpus(url='https://raw.githubusercontent.com/nlp-unibo/nlp-course-material/refs/heads/main/2024-2025/Assignment%201/data/training.json', filename='training.json')\n",
    "detector.download_corpus(url='https://raw.githubusercontent.com/nlp-unibo/nlp-course-material/refs/heads/main/2024-2025/Assignment%201/data/test.json', filename='test.json')\n",
    "detector.download_corpus(url='https://raw.githubusercontent.com/nlp-unibo/nlp-course-material/refs/heads/main/2024-2025/Assignment%201/data/validation.json', filename='validation.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the JSON files and encode them as a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = detector.load_corpus('training.json', orient='index', encoding='utf-8')\n",
    "test = detector.load_corpus('test.json', orient='index', encoding='utf-8')\n",
    "val = detector.load_corpus('validation.json', orient='index', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_EXIST</th>\n",
       "      <th>lang</th>\n",
       "      <th>tweet</th>\n",
       "      <th>number_annotators</th>\n",
       "      <th>annotators</th>\n",
       "      <th>gender_annotators</th>\n",
       "      <th>age_annotators</th>\n",
       "      <th>labels_task1</th>\n",
       "      <th>labels_task2</th>\n",
       "      <th>labels_task3</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100001</th>\n",
       "      <td>100001</td>\n",
       "      <td>es</td>\n",
       "      <td>@TheChiflis Ignora al otro, es un capullo.El p...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_1, Annotator_2, Annotator_3, Annota...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 46+, 23-45, 18-22]</td>\n",
       "      <td>[YES, YES, NO, YES, YES, YES]</td>\n",
       "      <td>[REPORTED, JUDGEMENTAL, -, REPORTED, JUDGEMENT...</td>\n",
       "      <td>[[OBJECTIFICATION], [OBJECTIFICATION, SEXUAL-V...</td>\n",
       "      <td>TRAIN_ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100002</th>\n",
       "      <td>100002</td>\n",
       "      <td>es</td>\n",
       "      <td>@ultimonomada_ Si comicsgate se parece en algo...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_7, Annotator_8, Annotator_9, Annota...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 46+, 23-45, 18-22]</td>\n",
       "      <td>[NO, NO, NO, NO, YES, NO]</td>\n",
       "      <td>[-, -, -, -, DIRECT, -]</td>\n",
       "      <td>[[-], [-], [-], [-], [OBJECTIFICATION], [-]]</td>\n",
       "      <td>TRAIN_ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100003</th>\n",
       "      <td>100003</td>\n",
       "      <td>es</td>\n",
       "      <td>@Steven2897 Lee sobre Gamergate, y como eso ha...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_7, Annotator_8, Annotator_9, Annota...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 46+, 23-45, 18-22]</td>\n",
       "      <td>[NO, NO, NO, NO, NO, NO]</td>\n",
       "      <td>[-, -, -, -, -, -]</td>\n",
       "      <td>[[-], [-], [-], [-], [-], [-]]</td>\n",
       "      <td>TRAIN_ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100004</th>\n",
       "      <td>100004</td>\n",
       "      <td>es</td>\n",
       "      <td>@Lunariita7 Un retraso social bastante lamenta...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_13, Annotator_14, Annotator_15, Ann...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 46+, 23-45, 18-22]</td>\n",
       "      <td>[NO, NO, YES, NO, YES, YES]</td>\n",
       "      <td>[-, -, DIRECT, -, REPORTED, REPORTED]</td>\n",
       "      <td>[[-], [-], [IDEOLOGICAL-INEQUALITY], [-], [IDE...</td>\n",
       "      <td>TRAIN_ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100005</th>\n",
       "      <td>100005</td>\n",
       "      <td>es</td>\n",
       "      <td>@novadragon21 @icep4ck @TvDannyZ Entonces como...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_19, Annotator_20, Annotator_21, Ann...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 46+, 23-45, 18-22]</td>\n",
       "      <td>[YES, NO, YES, NO, YES, YES]</td>\n",
       "      <td>[REPORTED, -, JUDGEMENTAL, -, JUDGEMENTAL, DIR...</td>\n",
       "      <td>[[STEREOTYPING-DOMINANCE, OBJECTIFICATION], [-...</td>\n",
       "      <td>TRAIN_ES</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id_EXIST lang                                              tweet  \\\n",
       "100001    100001   es  @TheChiflis Ignora al otro, es un capullo.El p...   \n",
       "100002    100002   es  @ultimonomada_ Si comicsgate se parece en algo...   \n",
       "100003    100003   es  @Steven2897 Lee sobre Gamergate, y como eso ha...   \n",
       "100004    100004   es  @Lunariita7 Un retraso social bastante lamenta...   \n",
       "100005    100005   es  @novadragon21 @icep4ck @TvDannyZ Entonces como...   \n",
       "\n",
       "        number_annotators                                         annotators  \\\n",
       "100001                  6  [Annotator_1, Annotator_2, Annotator_3, Annota...   \n",
       "100002                  6  [Annotator_7, Annotator_8, Annotator_9, Annota...   \n",
       "100003                  6  [Annotator_7, Annotator_8, Annotator_9, Annota...   \n",
       "100004                  6  [Annotator_13, Annotator_14, Annotator_15, Ann...   \n",
       "100005                  6  [Annotator_19, Annotator_20, Annotator_21, Ann...   \n",
       "\n",
       "         gender_annotators                          age_annotators  \\\n",
       "100001  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
       "100002  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
       "100003  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
       "100004  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
       "100005  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
       "\n",
       "                         labels_task1  \\\n",
       "100001  [YES, YES, NO, YES, YES, YES]   \n",
       "100002      [NO, NO, NO, NO, YES, NO]   \n",
       "100003       [NO, NO, NO, NO, NO, NO]   \n",
       "100004    [NO, NO, YES, NO, YES, YES]   \n",
       "100005   [YES, NO, YES, NO, YES, YES]   \n",
       "\n",
       "                                             labels_task2  \\\n",
       "100001  [REPORTED, JUDGEMENTAL, -, REPORTED, JUDGEMENT...   \n",
       "100002                            [-, -, -, -, DIRECT, -]   \n",
       "100003                                 [-, -, -, -, -, -]   \n",
       "100004              [-, -, DIRECT, -, REPORTED, REPORTED]   \n",
       "100005  [REPORTED, -, JUDGEMENTAL, -, JUDGEMENTAL, DIR...   \n",
       "\n",
       "                                             labels_task3     split  \n",
       "100001  [[OBJECTIFICATION], [OBJECTIFICATION, SEXUAL-V...  TRAIN_ES  \n",
       "100002       [[-], [-], [-], [-], [OBJECTIFICATION], [-]]  TRAIN_ES  \n",
       "100003                     [[-], [-], [-], [-], [-], [-]]  TRAIN_ES  \n",
       "100004  [[-], [-], [IDEOLOGICAL-INEQUALITY], [-], [IDE...  TRAIN_ES  \n",
       "100005  [[STEREOTYPING-DOMINANCE, OBJECTIFICATION], [-...  TRAIN_ES  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6920, 11), (726, 11), (312, 11))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, val.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate hard labels for Task 1 with majority voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['hard_label_task1'] = train['labels_task1'].apply(detector.majority_voting)\n",
    "val['hard_label_task1'] = val['labels_task1'].apply(detector.majority_voting)\n",
    "test['hard_label_task1'] = test['labels_task1'].apply(detector.majority_voting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter the DataFrame for only english tweets and remove unclear tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[(train['hard_label_task1'] != \"NEUTRAL\") & (train['lang'] == \"en\")]\n",
    "val = val[(val['hard_label_task1'] != \"NEUTRAL\") & (val['lang'] == \"en\")]\n",
    "test = test[(test['hard_label_task1'] != \"NEUTRAL\") & (test['lang'] == \"en\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2870, 12), (158, 12), (286, 12))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, val.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove unwanted columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[detector.columns_to_maintain]\n",
    "val = val[detector.columns_to_maintain]\n",
    "test = test[detector.columns_to_maintain]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_EXIST</th>\n",
       "      <th>lang</th>\n",
       "      <th>tweet</th>\n",
       "      <th>hard_label_task1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200002</th>\n",
       "      <td>200002</td>\n",
       "      <td>en</td>\n",
       "      <td>Writing a uni essay in my local pub with a cof...</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200003</th>\n",
       "      <td>200003</td>\n",
       "      <td>en</td>\n",
       "      <td>@UniversalORL it is 2021 not 1921. I dont appr...</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200006</th>\n",
       "      <td>200006</td>\n",
       "      <td>en</td>\n",
       "      <td>According to a customer I have plenty of time ...</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200007</th>\n",
       "      <td>200007</td>\n",
       "      <td>en</td>\n",
       "      <td>So only 'blokes' drink beer? Sorry, but if you...</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200008</th>\n",
       "      <td>200008</td>\n",
       "      <td>en</td>\n",
       "      <td>New to the shelves this week - looking forward...</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id_EXIST lang                                              tweet  \\\n",
       "200002    200002   en  Writing a uni essay in my local pub with a cof...   \n",
       "200003    200003   en  @UniversalORL it is 2021 not 1921. I dont appr...   \n",
       "200006    200006   en  According to a customer I have plenty of time ...   \n",
       "200007    200007   en  So only 'blokes' drink beer? Sorry, but if you...   \n",
       "200008    200008   en  New to the shelves this week - looking forward...   \n",
       "\n",
       "       hard_label_task1  \n",
       "200002              YES  \n",
       "200003              YES  \n",
       "200006              YES  \n",
       "200007              YES  \n",
       "200008               NO  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode the hard labels column as integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['hard_label_task1'] = train['hard_label_task1'].apply(lambda x: 1 if x == \"YES\" else 0)\n",
    "val['hard_label_task1'] = val['hard_label_task1'].apply(lambda x: 1 if x == \"YES\" else 0)\n",
    "test['hard_label_task1'] = test['hard_label_task1'].apply(lambda x: 1 if x == \"YES\" else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hard_label_task1\n",
       "0    1733\n",
       "1    1137\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.hard_label_task1.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Data Cleaning\n",
    "\n",
    "1. Remove emojis\n",
    "2. Remove hashtags (e.g. #metoo)\n",
    "3. Remove mentions (e.g. @user)\n",
    "4. Remove URLs\n",
    "5. Remove special characters and symbols\n",
    "6. Remove specific quote characters (e.g. curly quotes)\n",
    "7. Perform lemmatization\n",
    "\n",
    ">**Bonus**: use other preprocessing strategies exploring techniques tailored specifically for tweets or methods that are common in social media text processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way to go (priority order) is the following:\n",
    "1. Remove URLs\n",
    "2. Remove mentions\n",
    "3. Remove hashtags\n",
    "4. Remove emojis\n",
    "5. Remove special characters\n",
    "6. Remove specific quote characters\n",
    "7. Perform lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_train = train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['tweet'] = train['tweet'].apply(detector.preprocess_text)\n",
    "val['tweet'] = val['tweet'].apply(detector.preprocess_text)\n",
    "test['tweet'] = test['tweet'].apply(detector.preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['tweet'] = train['tweet'].apply(detector.lemmatize_text)\n",
    "val['tweet'] = val['tweet'].apply(detector.lemmatize_text)\n",
    "test['tweet'] = test['tweet'].apply(detector.lemmatize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the difference between the original and cleaned text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tweet:\n",
      "If I just got an A on my biology class watch out because my god complex will come back out so strong. My narcissism has been repressed for two weeks as I cried and struggled with my classes but IF I JUST GOT AN A ON MY BIO QUIZ JEEZ ID BE SO SMART\n",
      "Processed tweet:\n",
      "if i just got an a on my biology class watch out because my god complex will come back out so strong my narcissism ha been repressed for two week a i cried and struggled with my class but if i just got an a on my bio quiz jeez id be so smart\n"
     ]
    }
   ],
   "source": [
    "detector.text_diff(preprocessed_text=train, original_text=original_train, random=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Text Encoding\n",
    "\n",
    "* Embed words using GloVe embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embed words using GloVe embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_model = detector.load_glove(model_name='glove-twitter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx_to_word, train_word_to_idx = detector.get_vocab(train)\n",
    "train_word_listing = list(train_idx_to_word.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total OOV terms: 790 (8.20%)\n"
     ]
    }
   ],
   "source": [
    "detector.get_oov_stats(emb_model, train_word_listing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set(train_word_listing).difference(set(emb_model.key_to_index.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving vocab to data\\vocab2.json\n",
      "Vocab saved!\n"
     ]
    }
   ],
   "source": [
    "emb_model = detector.get_augmented_vocab(emb_model, train_words=train_word_listing, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_idx_to_word, val_word_to_idx = detector.get_vocab(val, word_listing=train_word_listing)\n",
    "test_idx_to_word, test_word_to_idx = detector.get_vocab(test, word_listing=train_word_listing)\n",
    "\n",
    "val_word_listing = list(val_idx_to_word.values())\n",
    "test_word_listing = list(test_idx_to_word.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\VSCODE\\Python\\Sexism-detection\\.venv\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:551: UserWarning: Adding single vectors to a KeyedVectors which grows by one each time can be costly. Consider adding in batches or preallocating to the required size.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1194304"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add the UNK token to the embedding model with the vector which is the average of all the vectors\n",
    "emb_model.add_vector(\"[UNK]\", np.mean(emb_model.vectors, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1194305, 50)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Create the embeddings matrix\n",
    "embedding_matrix = emb_model.vectors\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP\n",
    "reduced_embedding_umap = detector.reduce_umap(embedding_matrix)\n",
    "detector.visualize_embeddings(reduced_embedding_umap, ['whore', 'woman', 'slut', 'girl', 'man', 'boy'], emb_model.key_to_index)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5: Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 6: Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 7: Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 8: Report"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
