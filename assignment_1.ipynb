{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/MatteoFasulo/Sexism-detection/blob/main/assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from emoji import unicode_codes\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_PATTERN_STR = r\"\"\"(?i)((?:https?:(?:/{1,3}|[a-z0-9%])|[a-z0-9.\\-]+[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info\n",
    "                      |int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|\n",
    "                      bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|\n",
    "                      cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|\n",
    "                      gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|\n",
    "                      la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|\n",
    "                      nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|\n",
    "                      sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|\n",
    "                      uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)/)(?:[^\\s()<>{}\\[\\]]+|\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]\n",
    "                      *?\\)|\\([^\\s]+?\\))+(?:\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’])|(?:(?<!@)\n",
    "                      [a-z0-9]+(?:[.\\-][a-z0-9]+)*[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name\n",
    "                      |post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn\n",
    "                      |bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg\n",
    "                      |eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id\n",
    "                      |ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|\n",
    "                      md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|\n",
    "                      ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|\n",
    "                      sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|\n",
    "                      za|zm|zw)\\b/?(?!@)))\"\"\"\n",
    "URL_PATTERN = re.compile(URL_PATTERN_STR, re.IGNORECASE)\n",
    "HASHTAG_PATTERN = re.compile(r'#\\w*')\n",
    "MENTION_PATTERN = re.compile(r'@\\w*')\n",
    "RESERVED_WORDS_PATTERN = re.compile(r'\\b(?<![@#])(RT|FAV)\\b')\n",
    "EMOJIS_PATTERN = re.compile(u'([\\U00002600-\\U000027BF])|([\\U0001f300-\\U0001f64F])|([\\U0001f680-\\U0001f6FF])')\n",
    "SMILEYS_PATTERN = re.compile(r\"(\\s?:X|:|;|=)(?:-)?(?:\\)+|\\(|O|D|P|S|\\\\|\\/\\s){1,}\", re.IGNORECASE)\n",
    "SPECIAL_CHARACTERS_PATTERN = re.compile(r'&lt;/?[a-z]+&gt;')\n",
    "AND_PATTERN = re.compile(r'&amp;')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Corpus\n",
    "\n",
    "1. Download the data\n",
    "2. Load the JSON files and encode them as a DataFrame\n",
    "3. Generate hard labels for Task 1 with majority voting\n",
    "4. Filter the DataFrame for only english tweets\n",
    "5. Remove unwanted columns\n",
    "6. Encode the hard labels column as integers\n",
    "\n",
    ">**Bonus**: explore also Spanish tweets leveraging multi-language models and assessing the performance of the model on the two languages in comparison to the English-only model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not Path(\"data\").exists():\n",
    "    os.mkdir(\"data\")\n",
    "    print(\"data directory created\")\n",
    "\n",
    "train_url = 'https://raw.githubusercontent.com/nlp-unibo/nlp-course-material/refs/heads/main/2024-2025/Assignment%201/data/training.json'\n",
    "test_url = 'https://raw.githubusercontent.com/nlp-unibo/nlp-course-material/refs/heads/main/2024-2025/Assignment%201/data/test.json'\n",
    "val_url = 'https://raw.githubusercontent.com/nlp-unibo/nlp-course-material/refs/heads/main/2024-2025/Assignment%201/data/validation.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = requests.get(train_url)\n",
    "test = requests.get(test_url)\n",
    "val = requests.get(val_url)\n",
    "\n",
    "with open('data/training.json', 'w') as f:\n",
    "    f.write(train.text)\n",
    "\n",
    "with open('data/test.json', 'w') as f:\n",
    "    f.write(test.text)\n",
    "\n",
    "with open('data/validation.json', 'w') as f:\n",
    "    f.write(val.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the JSON files and encode them as a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_json(\"data/training.json\", orient='index', encoding='utf-8')\n",
    "val = pd.read_json(\"data/validation.json\", orient='index', encoding='utf-8')\n",
    "test = pd.read_json(\"data/test.json\", orient='index', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_EXIST</th>\n",
       "      <th>lang</th>\n",
       "      <th>tweet</th>\n",
       "      <th>number_annotators</th>\n",
       "      <th>annotators</th>\n",
       "      <th>gender_annotators</th>\n",
       "      <th>age_annotators</th>\n",
       "      <th>labels_task1</th>\n",
       "      <th>labels_task2</th>\n",
       "      <th>labels_task3</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100001</th>\n",
       "      <td>100001</td>\n",
       "      <td>es</td>\n",
       "      <td>@TheChiflis Ignora al otro, es un capullo.El p...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_1, Annotator_2, Annotator_3, Annota...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 46+, 23-45, 18-22]</td>\n",
       "      <td>[YES, YES, NO, YES, YES, YES]</td>\n",
       "      <td>[REPORTED, JUDGEMENTAL, -, REPORTED, JUDGEMENT...</td>\n",
       "      <td>[[OBJECTIFICATION], [OBJECTIFICATION, SEXUAL-V...</td>\n",
       "      <td>TRAIN_ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100002</th>\n",
       "      <td>100002</td>\n",
       "      <td>es</td>\n",
       "      <td>@ultimonomada_ Si comicsgate se parece en algo...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_7, Annotator_8, Annotator_9, Annota...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 46+, 23-45, 18-22]</td>\n",
       "      <td>[NO, NO, NO, NO, YES, NO]</td>\n",
       "      <td>[-, -, -, -, DIRECT, -]</td>\n",
       "      <td>[[-], [-], [-], [-], [OBJECTIFICATION], [-]]</td>\n",
       "      <td>TRAIN_ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100003</th>\n",
       "      <td>100003</td>\n",
       "      <td>es</td>\n",
       "      <td>@Steven2897 Lee sobre Gamergate, y como eso ha...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_7, Annotator_8, Annotator_9, Annota...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 46+, 23-45, 18-22]</td>\n",
       "      <td>[NO, NO, NO, NO, NO, NO]</td>\n",
       "      <td>[-, -, -, -, -, -]</td>\n",
       "      <td>[[-], [-], [-], [-], [-], [-]]</td>\n",
       "      <td>TRAIN_ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100004</th>\n",
       "      <td>100004</td>\n",
       "      <td>es</td>\n",
       "      <td>@Lunariita7 Un retraso social bastante lamenta...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_13, Annotator_14, Annotator_15, Ann...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 46+, 23-45, 18-22]</td>\n",
       "      <td>[NO, NO, YES, NO, YES, YES]</td>\n",
       "      <td>[-, -, DIRECT, -, REPORTED, REPORTED]</td>\n",
       "      <td>[[-], [-], [IDEOLOGICAL-INEQUALITY], [-], [IDE...</td>\n",
       "      <td>TRAIN_ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100005</th>\n",
       "      <td>100005</td>\n",
       "      <td>es</td>\n",
       "      <td>@novadragon21 @icep4ck @TvDannyZ Entonces como...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_19, Annotator_20, Annotator_21, Ann...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 46+, 23-45, 18-22]</td>\n",
       "      <td>[YES, NO, YES, NO, YES, YES]</td>\n",
       "      <td>[REPORTED, -, JUDGEMENTAL, -, JUDGEMENTAL, DIR...</td>\n",
       "      <td>[[STEREOTYPING-DOMINANCE, OBJECTIFICATION], [-...</td>\n",
       "      <td>TRAIN_ES</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id_EXIST lang                                              tweet  \\\n",
       "100001    100001   es  @TheChiflis Ignora al otro, es un capullo.El p...   \n",
       "100002    100002   es  @ultimonomada_ Si comicsgate se parece en algo...   \n",
       "100003    100003   es  @Steven2897 Lee sobre Gamergate, y como eso ha...   \n",
       "100004    100004   es  @Lunariita7 Un retraso social bastante lamenta...   \n",
       "100005    100005   es  @novadragon21 @icep4ck @TvDannyZ Entonces como...   \n",
       "\n",
       "        number_annotators                                         annotators  \\\n",
       "100001                  6  [Annotator_1, Annotator_2, Annotator_3, Annota...   \n",
       "100002                  6  [Annotator_7, Annotator_8, Annotator_9, Annota...   \n",
       "100003                  6  [Annotator_7, Annotator_8, Annotator_9, Annota...   \n",
       "100004                  6  [Annotator_13, Annotator_14, Annotator_15, Ann...   \n",
       "100005                  6  [Annotator_19, Annotator_20, Annotator_21, Ann...   \n",
       "\n",
       "         gender_annotators                          age_annotators  \\\n",
       "100001  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
       "100002  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
       "100003  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
       "100004  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
       "100005  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
       "\n",
       "                         labels_task1  \\\n",
       "100001  [YES, YES, NO, YES, YES, YES]   \n",
       "100002      [NO, NO, NO, NO, YES, NO]   \n",
       "100003       [NO, NO, NO, NO, NO, NO]   \n",
       "100004    [NO, NO, YES, NO, YES, YES]   \n",
       "100005   [YES, NO, YES, NO, YES, YES]   \n",
       "\n",
       "                                             labels_task2  \\\n",
       "100001  [REPORTED, JUDGEMENTAL, -, REPORTED, JUDGEMENT...   \n",
       "100002                            [-, -, -, -, DIRECT, -]   \n",
       "100003                                 [-, -, -, -, -, -]   \n",
       "100004              [-, -, DIRECT, -, REPORTED, REPORTED]   \n",
       "100005  [REPORTED, -, JUDGEMENTAL, -, JUDGEMENTAL, DIR...   \n",
       "\n",
       "                                             labels_task3     split  \n",
       "100001  [[OBJECTIFICATION], [OBJECTIFICATION, SEXUAL-V...  TRAIN_ES  \n",
       "100002       [[-], [-], [-], [-], [OBJECTIFICATION], [-]]  TRAIN_ES  \n",
       "100003                     [[-], [-], [-], [-], [-], [-]]  TRAIN_ES  \n",
       "100004  [[-], [-], [IDEOLOGICAL-INEQUALITY], [-], [IDE...  TRAIN_ES  \n",
       "100005  [[STEREOTYPING-DOMINANCE, OBJECTIFICATION], [-...  TRAIN_ES  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6920, 11), (726, 11), (312, 11))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, val.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate hard labels for Task 1 with majority voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_voting(votes: list[str]) -> str:\n",
    "    total_num_votes = len(votes)\n",
    "    yes_votes = votes.count(\"YES\")\n",
    "    no_votes = total_num_votes - yes_votes\n",
    "\n",
    "    if yes_votes > no_votes:\n",
    "        return \"YES\"\n",
    "    elif no_votes > yes_votes:\n",
    "        return \"NO\"\n",
    "    else:\n",
    "        return \"NEUTRAL\" # This will be the case when there is a tie (removed later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['hard_label_task1'] = train['labels_task1'].apply(majority_voting)\n",
    "val['hard_label_task1'] = val['labels_task1'].apply(majority_voting)\n",
    "test['hard_label_task1'] = test['labels_task1'].apply(majority_voting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter the DataFrame for only english tweets and remove unclear tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[(train['hard_label_task1'] != \"NEUTRAL\") & (train['lang'] == \"en\")]\n",
    "val = val[(val['hard_label_task1'] != \"NEUTRAL\") & (val['lang'] == \"en\")]\n",
    "test = test[(test['hard_label_task1'] != \"NEUTRAL\") & (test['lang'] == \"en\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2870, 12), (158, 12), (286, 12))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, val.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove unwanted columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_maintain = ['id_EXIST', 'lang', 'tweet', 'hard_label_task1']\n",
    "\n",
    "train = train[columns_to_maintain]\n",
    "val = val[columns_to_maintain]\n",
    "test = test[columns_to_maintain]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_EXIST</th>\n",
       "      <th>lang</th>\n",
       "      <th>tweet</th>\n",
       "      <th>hard_label_task1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200002</th>\n",
       "      <td>200002</td>\n",
       "      <td>en</td>\n",
       "      <td>Writing a uni essay in my local pub with a cof...</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200003</th>\n",
       "      <td>200003</td>\n",
       "      <td>en</td>\n",
       "      <td>@UniversalORL it is 2021 not 1921. I dont appr...</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200006</th>\n",
       "      <td>200006</td>\n",
       "      <td>en</td>\n",
       "      <td>According to a customer I have plenty of time ...</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200007</th>\n",
       "      <td>200007</td>\n",
       "      <td>en</td>\n",
       "      <td>So only 'blokes' drink beer? Sorry, but if you...</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200008</th>\n",
       "      <td>200008</td>\n",
       "      <td>en</td>\n",
       "      <td>New to the shelves this week - looking forward...</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id_EXIST lang                                              tweet  \\\n",
       "200002    200002   en  Writing a uni essay in my local pub with a cof...   \n",
       "200003    200003   en  @UniversalORL it is 2021 not 1921. I dont appr...   \n",
       "200006    200006   en  According to a customer I have plenty of time ...   \n",
       "200007    200007   en  So only 'blokes' drink beer? Sorry, but if you...   \n",
       "200008    200008   en  New to the shelves this week - looking forward...   \n",
       "\n",
       "       hard_label_task1  \n",
       "200002              YES  \n",
       "200003              YES  \n",
       "200006              YES  \n",
       "200007              YES  \n",
       "200008               NO  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode the hard labels column as integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['hard_label_task1'] = train['hard_label_task1'].apply(lambda x: 1 if x == \"YES\" else 0)\n",
    "val['hard_label_task1'] = val['hard_label_task1'].apply(lambda x: 1 if x == \"YES\" else 0)\n",
    "test['hard_label_task1'] = test['hard_label_task1'].apply(lambda x: 1 if x == \"YES\" else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hard_label_task1\n",
       "0    1733\n",
       "1    1137\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.hard_label_task1.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Data Cleaning\n",
    "\n",
    "1. Remove emojis\n",
    "2. Remove hashtags (e.g. #metoo)\n",
    "3. Remove mentions (e.g. @user)\n",
    "4. Remove URLs\n",
    "5. Remove special characters and symbols\n",
    "6. Remove specific quote characters (e.g. curly quotes)\n",
    "7. Perform lemmatization\n",
    "\n",
    ">**Bonus**: use other preprocessing strategies exploring techniques tailored specifically for tweets or methods that are common in social media text processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way to go (priority order) is the following:\n",
    "1. Remove URLs\n",
    "2. Remove mentions\n",
    "3. Remove hashtags\n",
    "4. Remove emojis\n",
    "5. Remove special characters\n",
    "6. Remove specific quote characters\n",
    "7. Perform lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_train = train.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['tweet'] = train['tweet'].apply(lambda x: URL_PATTERN.sub('', x))\n",
    "val['tweet'] = val['tweet'].apply(lambda x: URL_PATTERN.sub('', x))\n",
    "test['tweet'] = test['tweet'].apply(lambda x: URL_PATTERN.sub('', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['tweet'] = train['tweet'].apply(lambda x: MENTION_PATTERN.sub('', x))\n",
    "val['tweet'] = val['tweet'].apply(lambda x: MENTION_PATTERN.sub('', x))\n",
    "test['tweet'] = test['tweet'].apply(lambda x: MENTION_PATTERN.sub('', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['tweet'] = train['tweet'].apply(lambda x: HASHTAG_PATTERN.sub('', x))\n",
    "val['tweet'] = val['tweet'].apply(lambda x: HASHTAG_PATTERN.sub('', x))\n",
    "test['tweet'] = test['tweet'].apply(lambda x: HASHTAG_PATTERN.sub('', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove emojis\n",
    "\n",
    "Leveraging the `emoji` library, it is possible to retrieve all the emojis codes. Then, we can remove them from the text very easily.\n",
    ">**Note**: the `emoji` library has also a `replace_emoji()` function that can be used to replace the emojis with a specific string. However, for the purpose of this task, we will remove them iterating over the text instead of using the `replace_emoji()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(text: str):\n",
    "    emojis = unicode_codes.EMOJI_DATA\n",
    "    result = text\n",
    "    for emoji in emojis:\n",
    "        result = result.replace(emoji, \"\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['tweet'] = train['tweet'].apply(remove_emoji)\n",
    "val['tweet'] = val['tweet'].apply(remove_emoji)\n",
    "test['tweet'] = test['tweet'].apply(remove_emoji)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['tweet'] = train['tweet'].apply(lambda x: SPECIAL_CHARACTERS_PATTERN.sub('', x))\n",
    "val['tweet'] = val['tweet'].apply(lambda x: SPECIAL_CHARACTERS_PATTERN.sub('', x))\n",
    "test['tweet'] = test['tweet'].apply(lambda x: SPECIAL_CHARACTERS_PATTERN.sub('', x))\n",
    "\n",
    "train['tweet'] = train['tweet'].apply(lambda x: AND_PATTERN.sub('and', x))\n",
    "val['tweet'] = val['tweet'].apply(lambda x: AND_PATTERN.sub('and', x))\n",
    "test['tweet'] = test['tweet'].apply(lambda x: AND_PATTERN.sub('and', x))\n",
    "\n",
    "train['tweet'] = train['tweet'].str.strip()\n",
    "val['tweet'] = val['tweet'].str.strip()\n",
    "test['tweet'] = test['tweet'].str.strip()\n",
    "\n",
    "# Leave only words, remove any other special character, symbol or specific quote character\n",
    "train['tweet'] = train['tweet'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "val['tweet'] = val['tweet'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "test['tweet'] = test['tweet'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove specific quote characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'why do you look like a whore lh'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmatizer.lemmatize(train['tweet'].iloc[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@clintneedcoffee why do you look like a whore? /lh https://t.co/9HOUvqBXMV'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_train['tweet'].iloc[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_diff(random: bool = True):\n",
    "    if random:\n",
    "        idx = np.random.randint(0, train.shape[0])\n",
    "    else:\n",
    "        idx = 0\n",
    "\n",
    "    print(f\"Original tweet:\\n{original_train['tweet'].iloc[idx]}\")\n",
    "    print(f\"Processed tweet:\\n{train['tweet'].iloc[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tweet:\n",
      "@KikiwahT My big takeaway was them saying trust the science but you're right characters were portrayed. Alot of stereotyping took place.Meryl Streep was essentially portraying their version of Donald Trump in the movie.\n",
      "Processed tweet:\n",
      "My big takeaway was them saying trust the science but youre right characters were portrayed Alot of stereotyping took placeMeryl Streep was essentially portraying their version of Donald Trump in the movie\n"
     ]
    }
   ],
   "source": [
    "show_diff()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Text Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5: Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 6: Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 7: Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 8: Report"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
