{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/MatteoFasulo/Sexism-detection/blob/main/assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from pathlib import Path\n",
    "import re\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from emoji import unicode_codes\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import gensim\n",
    "import gensim.downloader as gloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_PATTERN_STR = r\"\"\"(?i)((?:https?:(?:/{1,3}|[a-z0-9%])|[a-z0-9.\\-]+[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info\n",
    "                      |int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|\n",
    "                      bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|\n",
    "                      cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|\n",
    "                      gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|\n",
    "                      la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|\n",
    "                      nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|\n",
    "                      sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|\n",
    "                      uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)/)(?:[^\\s()<>{}\\[\\]]+|\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]\n",
    "                      *?\\)|\\([^\\s]+?\\))+(?:\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’])|(?:(?<!@)\n",
    "                      [a-z0-9]+(?:[.\\-][a-z0-9]+)*[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name\n",
    "                      |post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn\n",
    "                      |bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg\n",
    "                      |eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id\n",
    "                      |ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|\n",
    "                      md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|\n",
    "                      ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|\n",
    "                      sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|\n",
    "                      za|zm|zw)\\b/?(?!@)))\"\"\"\n",
    "URL_PATTERN = re.compile(URL_PATTERN_STR, re.IGNORECASE)\n",
    "HASHTAG_PATTERN = re.compile(r'#\\w*')\n",
    "MENTION_PATTERN = re.compile(r'@\\w*')\n",
    "RESERVED_WORDS_PATTERN = re.compile(r'\\b(?<![@#])(RT|FAV)\\b')\n",
    "EMOJIS_PATTERN = re.compile(u'([\\U00002600-\\U000027BF])|([\\U0001f300-\\U0001f64F])|([\\U0001f680-\\U0001f6FF])')\n",
    "SMILEYS_PATTERN = re.compile(r\"(\\s?:X|:|;|=)(?:-)?(?:\\)+|\\(|O|D|P|S|\\\\|\\/\\s){1,}\", re.IGNORECASE)\n",
    "SPECIAL_CHARACTERS_PATTERN = re.compile(r'&lt;/?[a-z]+&gt;')\n",
    "AND_PATTERN = re.compile(r'&amp;')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Corpus\n",
    "\n",
    "1. Download the data\n",
    "2. Load the JSON files and encode them as a DataFrame\n",
    "3. Generate hard labels for Task 1 with majority voting\n",
    "4. Filter the DataFrame for only english tweets\n",
    "5. Remove unwanted columns\n",
    "6. Encode the hard labels column as integers\n",
    "\n",
    ">**Bonus**: explore also Spanish tweets leveraging multi-language models and assessing the performance of the model on the two languages in comparison to the English-only model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not Path(\"data\").exists():\n",
    "    os.mkdir(\"data\")\n",
    "    print(\"data directory created\")\n",
    "\n",
    "train_url = 'https://raw.githubusercontent.com/nlp-unibo/nlp-course-material/refs/heads/main/2024-2025/Assignment%201/data/training.json'\n",
    "test_url = 'https://raw.githubusercontent.com/nlp-unibo/nlp-course-material/refs/heads/main/2024-2025/Assignment%201/data/test.json'\n",
    "val_url = 'https://raw.githubusercontent.com/nlp-unibo/nlp-course-material/refs/heads/main/2024-2025/Assignment%201/data/validation.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = requests.get(train_url)\n",
    "test = requests.get(test_url)\n",
    "val = requests.get(val_url)\n",
    "\n",
    "with open('data/training.json', 'w') as f:\n",
    "    f.write(train.text)\n",
    "\n",
    "with open('data/test.json', 'w') as f:\n",
    "    f.write(test.text)\n",
    "\n",
    "with open('data/validation.json', 'w') as f:\n",
    "    f.write(val.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the JSON files and encode them as a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_json(\"data/training.json\", orient='index', encoding='utf-8')\n",
    "val = pd.read_json(\"data/validation.json\", orient='index', encoding='utf-8')\n",
    "test = pd.read_json(\"data/test.json\", orient='index', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_EXIST</th>\n",
       "      <th>lang</th>\n",
       "      <th>tweet</th>\n",
       "      <th>number_annotators</th>\n",
       "      <th>annotators</th>\n",
       "      <th>gender_annotators</th>\n",
       "      <th>age_annotators</th>\n",
       "      <th>labels_task1</th>\n",
       "      <th>labels_task2</th>\n",
       "      <th>labels_task3</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100001</th>\n",
       "      <td>100001</td>\n",
       "      <td>es</td>\n",
       "      <td>@TheChiflis Ignora al otro, es un capullo.El p...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_1, Annotator_2, Annotator_3, Annota...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 46+, 23-45, 18-22]</td>\n",
       "      <td>[YES, YES, NO, YES, YES, YES]</td>\n",
       "      <td>[REPORTED, JUDGEMENTAL, -, REPORTED, JUDGEMENT...</td>\n",
       "      <td>[[OBJECTIFICATION], [OBJECTIFICATION, SEXUAL-V...</td>\n",
       "      <td>TRAIN_ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100002</th>\n",
       "      <td>100002</td>\n",
       "      <td>es</td>\n",
       "      <td>@ultimonomada_ Si comicsgate se parece en algo...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_7, Annotator_8, Annotator_9, Annota...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 46+, 23-45, 18-22]</td>\n",
       "      <td>[NO, NO, NO, NO, YES, NO]</td>\n",
       "      <td>[-, -, -, -, DIRECT, -]</td>\n",
       "      <td>[[-], [-], [-], [-], [OBJECTIFICATION], [-]]</td>\n",
       "      <td>TRAIN_ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100003</th>\n",
       "      <td>100003</td>\n",
       "      <td>es</td>\n",
       "      <td>@Steven2897 Lee sobre Gamergate, y como eso ha...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_7, Annotator_8, Annotator_9, Annota...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 46+, 23-45, 18-22]</td>\n",
       "      <td>[NO, NO, NO, NO, NO, NO]</td>\n",
       "      <td>[-, -, -, -, -, -]</td>\n",
       "      <td>[[-], [-], [-], [-], [-], [-]]</td>\n",
       "      <td>TRAIN_ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100004</th>\n",
       "      <td>100004</td>\n",
       "      <td>es</td>\n",
       "      <td>@Lunariita7 Un retraso social bastante lamenta...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_13, Annotator_14, Annotator_15, Ann...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 46+, 23-45, 18-22]</td>\n",
       "      <td>[NO, NO, YES, NO, YES, YES]</td>\n",
       "      <td>[-, -, DIRECT, -, REPORTED, REPORTED]</td>\n",
       "      <td>[[-], [-], [IDEOLOGICAL-INEQUALITY], [-], [IDE...</td>\n",
       "      <td>TRAIN_ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100005</th>\n",
       "      <td>100005</td>\n",
       "      <td>es</td>\n",
       "      <td>@novadragon21 @icep4ck @TvDannyZ Entonces como...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_19, Annotator_20, Annotator_21, Ann...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 46+, 23-45, 18-22]</td>\n",
       "      <td>[YES, NO, YES, NO, YES, YES]</td>\n",
       "      <td>[REPORTED, -, JUDGEMENTAL, -, JUDGEMENTAL, DIR...</td>\n",
       "      <td>[[STEREOTYPING-DOMINANCE, OBJECTIFICATION], [-...</td>\n",
       "      <td>TRAIN_ES</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id_EXIST lang                                              tweet  \\\n",
       "100001    100001   es  @TheChiflis Ignora al otro, es un capullo.El p...   \n",
       "100002    100002   es  @ultimonomada_ Si comicsgate se parece en algo...   \n",
       "100003    100003   es  @Steven2897 Lee sobre Gamergate, y como eso ha...   \n",
       "100004    100004   es  @Lunariita7 Un retraso social bastante lamenta...   \n",
       "100005    100005   es  @novadragon21 @icep4ck @TvDannyZ Entonces como...   \n",
       "\n",
       "        number_annotators                                         annotators  \\\n",
       "100001                  6  [Annotator_1, Annotator_2, Annotator_3, Annota...   \n",
       "100002                  6  [Annotator_7, Annotator_8, Annotator_9, Annota...   \n",
       "100003                  6  [Annotator_7, Annotator_8, Annotator_9, Annota...   \n",
       "100004                  6  [Annotator_13, Annotator_14, Annotator_15, Ann...   \n",
       "100005                  6  [Annotator_19, Annotator_20, Annotator_21, Ann...   \n",
       "\n",
       "         gender_annotators                          age_annotators  \\\n",
       "100001  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
       "100002  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
       "100003  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
       "100004  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
       "100005  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
       "\n",
       "                         labels_task1  \\\n",
       "100001  [YES, YES, NO, YES, YES, YES]   \n",
       "100002      [NO, NO, NO, NO, YES, NO]   \n",
       "100003       [NO, NO, NO, NO, NO, NO]   \n",
       "100004    [NO, NO, YES, NO, YES, YES]   \n",
       "100005   [YES, NO, YES, NO, YES, YES]   \n",
       "\n",
       "                                             labels_task2  \\\n",
       "100001  [REPORTED, JUDGEMENTAL, -, REPORTED, JUDGEMENT...   \n",
       "100002                            [-, -, -, -, DIRECT, -]   \n",
       "100003                                 [-, -, -, -, -, -]   \n",
       "100004              [-, -, DIRECT, -, REPORTED, REPORTED]   \n",
       "100005  [REPORTED, -, JUDGEMENTAL, -, JUDGEMENTAL, DIR...   \n",
       "\n",
       "                                             labels_task3     split  \n",
       "100001  [[OBJECTIFICATION], [OBJECTIFICATION, SEXUAL-V...  TRAIN_ES  \n",
       "100002       [[-], [-], [-], [-], [OBJECTIFICATION], [-]]  TRAIN_ES  \n",
       "100003                     [[-], [-], [-], [-], [-], [-]]  TRAIN_ES  \n",
       "100004  [[-], [-], [IDEOLOGICAL-INEQUALITY], [-], [IDE...  TRAIN_ES  \n",
       "100005  [[STEREOTYPING-DOMINANCE, OBJECTIFICATION], [-...  TRAIN_ES  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6920, 11), (726, 11), (312, 11))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, val.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate hard labels for Task 1 with majority voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_voting(votes: list[str]) -> str:\n",
    "    total_num_votes = len(votes)\n",
    "    yes_votes = votes.count(\"YES\")\n",
    "    no_votes = total_num_votes - yes_votes\n",
    "\n",
    "    if yes_votes > no_votes:\n",
    "        return \"YES\"\n",
    "    elif no_votes > yes_votes:\n",
    "        return \"NO\"\n",
    "    else:\n",
    "        return \"NEUTRAL\" # This will be the case when there is a tie (removed later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['hard_label_task1'] = train['labels_task1'].apply(majority_voting)\n",
    "val['hard_label_task1'] = val['labels_task1'].apply(majority_voting)\n",
    "test['hard_label_task1'] = test['labels_task1'].apply(majority_voting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter the DataFrame for only english tweets and remove unclear tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[(train['hard_label_task1'] != \"NEUTRAL\") & (train['lang'] == \"en\")]\n",
    "val = val[(val['hard_label_task1'] != \"NEUTRAL\") & (val['lang'] == \"en\")]\n",
    "test = test[(test['hard_label_task1'] != \"NEUTRAL\") & (test['lang'] == \"en\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2870, 12), (158, 12), (286, 12))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, val.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove unwanted columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_maintain = ['id_EXIST', 'lang', 'tweet', 'hard_label_task1']\n",
    "\n",
    "train = train[columns_to_maintain]\n",
    "val = val[columns_to_maintain]\n",
    "test = test[columns_to_maintain]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_EXIST</th>\n",
       "      <th>lang</th>\n",
       "      <th>tweet</th>\n",
       "      <th>hard_label_task1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200002</th>\n",
       "      <td>200002</td>\n",
       "      <td>en</td>\n",
       "      <td>Writing a uni essay in my local pub with a cof...</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200003</th>\n",
       "      <td>200003</td>\n",
       "      <td>en</td>\n",
       "      <td>@UniversalORL it is 2021 not 1921. I dont appr...</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200006</th>\n",
       "      <td>200006</td>\n",
       "      <td>en</td>\n",
       "      <td>According to a customer I have plenty of time ...</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200007</th>\n",
       "      <td>200007</td>\n",
       "      <td>en</td>\n",
       "      <td>So only 'blokes' drink beer? Sorry, but if you...</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200008</th>\n",
       "      <td>200008</td>\n",
       "      <td>en</td>\n",
       "      <td>New to the shelves this week - looking forward...</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id_EXIST lang                                              tweet  \\\n",
       "200002    200002   en  Writing a uni essay in my local pub with a cof...   \n",
       "200003    200003   en  @UniversalORL it is 2021 not 1921. I dont appr...   \n",
       "200006    200006   en  According to a customer I have plenty of time ...   \n",
       "200007    200007   en  So only 'blokes' drink beer? Sorry, but if you...   \n",
       "200008    200008   en  New to the shelves this week - looking forward...   \n",
       "\n",
       "       hard_label_task1  \n",
       "200002              YES  \n",
       "200003              YES  \n",
       "200006              YES  \n",
       "200007              YES  \n",
       "200008               NO  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode the hard labels column as integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['hard_label_task1'] = train['hard_label_task1'].apply(lambda x: 1 if x == \"YES\" else 0)\n",
    "val['hard_label_task1'] = val['hard_label_task1'].apply(lambda x: 1 if x == \"YES\" else 0)\n",
    "test['hard_label_task1'] = test['hard_label_task1'].apply(lambda x: 1 if x == \"YES\" else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hard_label_task1\n",
       "0    1733\n",
       "1    1137\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.hard_label_task1.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Data Cleaning\n",
    "\n",
    "1. Remove emojis\n",
    "2. Remove hashtags (e.g. #metoo)\n",
    "3. Remove mentions (e.g. @user)\n",
    "4. Remove URLs\n",
    "5. Remove special characters and symbols\n",
    "6. Remove specific quote characters (e.g. curly quotes)\n",
    "7. Perform lemmatization\n",
    "\n",
    ">**Bonus**: use other preprocessing strategies exploring techniques tailored specifically for tweets or methods that are common in social media text processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way to go (priority order) is the following:\n",
    "1. Remove URLs\n",
    "2. Remove mentions\n",
    "3. Remove hashtags\n",
    "4. Remove emojis\n",
    "5. Remove special characters\n",
    "6. Remove specific quote characters\n",
    "7. Perform lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_train = train.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['tweet'] = train['tweet'].apply(lambda x: URL_PATTERN.sub('', x))\n",
    "val['tweet'] = val['tweet'].apply(lambda x: URL_PATTERN.sub('', x))\n",
    "test['tweet'] = test['tweet'].apply(lambda x: URL_PATTERN.sub('', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['tweet'] = train['tweet'].apply(lambda x: MENTION_PATTERN.sub('', x))\n",
    "val['tweet'] = val['tweet'].apply(lambda x: MENTION_PATTERN.sub('', x))\n",
    "test['tweet'] = test['tweet'].apply(lambda x: MENTION_PATTERN.sub('', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['tweet'] = train['tweet'].apply(lambda x: HASHTAG_PATTERN.sub('', x))\n",
    "val['tweet'] = val['tweet'].apply(lambda x: HASHTAG_PATTERN.sub('', x))\n",
    "test['tweet'] = test['tweet'].apply(lambda x: HASHTAG_PATTERN.sub('', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove emojis\n",
    "\n",
    "Leveraging the `emoji` library, it is possible to retrieve all the emojis codes. Then, we can remove them from the text very easily.\n",
    ">**Note**: the `emoji` library has also a `replace_emoji()` function that can be used to replace the emojis with a specific string. However, for the purpose of this task, we will remove them iterating over the text instead of using the `replace_emoji()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(text: str):\n",
    "    emojis = unicode_codes.EMOJI_DATA\n",
    "    result = text\n",
    "    for emoji in emojis:\n",
    "        result = result.replace(emoji, \"\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['tweet'] = train['tweet'].apply(remove_emoji)\n",
    "val['tweet'] = val['tweet'].apply(remove_emoji)\n",
    "test['tweet'] = test['tweet'].apply(remove_emoji)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['tweet'] = train['tweet'].apply(lambda x: SPECIAL_CHARACTERS_PATTERN.sub(' ', x))\n",
    "val['tweet'] = val['tweet'].apply(lambda x: SPECIAL_CHARACTERS_PATTERN.sub(' ', x))\n",
    "test['tweet'] = test['tweet'].apply(lambda x: SPECIAL_CHARACTERS_PATTERN.sub(' ', x))\n",
    "\n",
    "train['tweet'] = train['tweet'].apply(lambda x: AND_PATTERN.sub('and', x))\n",
    "val['tweet'] = val['tweet'].apply(lambda x: AND_PATTERN.sub('and', x))\n",
    "test['tweet'] = test['tweet'].apply(lambda x: AND_PATTERN.sub('and', x))\n",
    "\n",
    "train['tweet'] = train['tweet'].str.strip()\n",
    "val['tweet'] = val['tweet'].str.strip()\n",
    "test['tweet'] = test['tweet'].str.strip()\n",
    "\n",
    "# Leave only words, remove any other special character, symbol or specific quote character\n",
    "train['tweet'] = train['tweet'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "val['tweet'] = val['tweet'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "test['tweet'] = test['tweet'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove specific quote characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ik when mandy says you look like a whore i look cute as FUCK'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['tweet'] = train['tweet'].apply(lambda x: lemmatizer.lemmatize(x))\n",
    "val['tweet'] = val['tweet'].apply(lambda x: lemmatizer.lemmatize(x))\n",
    "test['tweet'] = test['tweet'].apply(lambda x: lemmatizer.lemmatize(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put everything lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['tweet'] = train['tweet'].apply(lambda x: x.lower())\n",
    "val['tweet'] = val['tweet'].apply(lambda x: x.lower())\n",
    "test['tweet'] = test['tweet'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tweet:\n",
      "@GBPdaily Aaron Rodgers has stood alone against all the bullshit thrown at him and rose above all of the CowPatties. He refused to be pushed and bullied to do something that goes against what worked for him. Go Aaron\n",
      "Processed tweet:\n",
      "aaron rodgers has stood alone against all the bullshit thrown at him and rose above all of the cowpatties he refused to be pushed and bullied to do something that goes against what worked for him go aaron\n"
     ]
    }
   ],
   "source": [
    "def show_diff(random: bool = True):\n",
    "    if random:\n",
    "        idx = np.random.randint(0, train.shape[0])\n",
    "    else:\n",
    "        idx = 0\n",
    "\n",
    "    print(f\"Original tweet:\\n{original_train['tweet'].iloc[idx]}\")\n",
    "    print(f\"Processed tweet:\\n{train['tweet'].iloc[idx]}\")\n",
    "    \n",
    "show_diff()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Text Encoding\n",
    "\n",
    "* Embed words using GloVe embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embed words using GloVe embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 50\n",
    "model = 'glove-wiki-gigaword'\n",
    "emb_model = gloader.load(f\"{model}-{dim}\")\n",
    "unk_embedding = np.zeros(dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import OrderedDict\n",
    "\n",
    "\n",
    "def get_vocab(data: pd.DataFrame) -> OrderedDict:\n",
    "    idx_to_word = OrderedDict()\n",
    "    word_to_idx = OrderedDict()\n",
    "\n",
    "    curr_idx = 0\n",
    "    for sentence in data['tweet'].values:\n",
    "        tokens = sentence.split()\n",
    "        for token in tokens:\n",
    "            if token not in word_to_idx:\n",
    "                word_to_idx[token] = curr_idx\n",
    "                idx_to_word[curr_idx] = token\n",
    "                curr_idx += 1\n",
    "\n",
    "    return idx_to_word, word_to_idx\n",
    "\n",
    "train_idx_to_word, train_word_to_idx = get_vocab(train)\n",
    "train_word_listing = list(train_idx_to_word.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a given token if it is in the training set we add that to the vocabulary and assign the corresponding embedding using GloVe or Custom embeddings. If, instead, it is in the validation or test set we assign the special token [UNK] (if not already present in the vocabulary) and assign a custom embedding\n",
    "\n",
    "#all_tokens = set(train_word_listing + val_word_listing + test_word_listing)\n",
    "embedding_vocabulary = set(emb_model.key_to_index.keys()) # GloVe Vocabulary\n",
    "\n",
    "new_tokens = []\n",
    "new_vectors = []\n",
    "for token in train_word_listing:\n",
    "    # Add to the vocabulary if not already present\n",
    "    if token not in embedding_vocabulary:\n",
    "        #print(f\"Token {token} not in GloVe vocabulary\") # TODO: check data processing\n",
    "        embedding_vocabulary.add(token)\n",
    "    try:\n",
    "        embedding_vector = emb_model.get_vector(token)\n",
    "    except (KeyError, ValueError):\n",
    "        embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=dim)\n",
    "\n",
    "    new_tokens.append(token)\n",
    "    new_vectors.append(embedding_vector)\n",
    "\n",
    "emb_model.add_vectors(new_tokens, new_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_val_test(data, train_word_listing):\n",
    "    idx_to_word = OrderedDict()\n",
    "    word_to_idx = OrderedDict()\n",
    "\n",
    "    unk_token = \"[UNK]\"\n",
    "    curr_idx = 1\n",
    "\n",
    "    word_to_idx[unk_token] = 0\n",
    "    idx_to_word[0] = unk_token\n",
    "\n",
    "    for sentence in data['tweet'].values:\n",
    "        tokens = sentence.split()\n",
    "        for token in tokens:\n",
    "            if token in train_word_listing:\n",
    "                word_to_idx[token] = curr_idx\n",
    "                idx_to_word[curr_idx] = token\n",
    "                curr_idx += 1\n",
    "\n",
    "    return idx_to_word, word_to_idx\n",
    "\n",
    "val_idx_to_word, val_word_to_idx = get_vocab_val_test(val, train_word_listing)\n",
    "test_idx_to_word, test_word_to_idx = get_vocab_val_test(test, train_word_listing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "401881"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_model.add_vector(\"[UNK]\", unk_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create the embeddings matrix\n",
    "embedding_matrix = emb_model.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "def visualize_embeddings(embeddings: np.ndarray,\n",
    "                         word_annotations,\n",
    "                         word_to_idx):\n",
    "    \"\"\"\n",
    "    Plots given reduce word embeddings (2D). Users can highlight specific words (word_annotations list).\n",
    "\n",
    "    :param embeddings: word embedding matrix of shape (words, 2) retrieved via a dimensionality reduction technique.\n",
    "    :param word_annotations: list of words to be annotated.\n",
    "    :param word_to_idx: vocabulary map (word -> index) (dict)\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(15, 12))\n",
    "\n",
    "    if word_annotations:\n",
    "        print(f\"Annotating words: {word_annotations}\")\n",
    "\n",
    "        word_indexes = []\n",
    "        for word in word_annotations:\n",
    "            word_index = word_to_idx[word]\n",
    "            word_indexes.append(word_index)\n",
    "\n",
    "        word_indexes = np.array(word_indexes)\n",
    "\n",
    "        other_embeddings = embeddings[np.setdiff1d(np.arange(embeddings.shape[0]), word_indexes)]\n",
    "        target_embeddings = embeddings[word_indexes]\n",
    "\n",
    "        ax.scatter(other_embeddings[:, 0], other_embeddings[:, 1], alpha=0.1, c='blue')\n",
    "        ax.scatter(target_embeddings[:, 0], target_embeddings[:, 1], alpha=1.0, c='red')\n",
    "        ax.scatter(target_embeddings[:, 0], target_embeddings[:, 1], alpha=1, facecolors='none', edgecolors='r', s=1000)\n",
    "\n",
    "        for word, word_index in zip(word_annotations, word_indexes):\n",
    "            word_x, word_y = embeddings[word_index, 0], embeddings[word_index, 1]\n",
    "            ax.annotate(word, xy=(word_x, word_y))\n",
    "    else:\n",
    "        ax.scatter(embeddings[:, 0], embeddings[:, 1], alpha=0.1, c='blue')\n",
    "\n",
    "    # We avoid outliers ruining the visualization if they are quite far away\n",
    "    axis_x_limit = (np.min(embeddings[:, 0]), np.max(embeddings[:, 0]))\n",
    "    axis_y_limit = (np.min(embeddings[:, 1]), np.max(embeddings[:, 1]))\n",
    "    plt.xlim(left=axis_x_limit[0] - 0.5, right=axis_x_limit[1] + 0.5)\n",
    "    plt.ylim(bottom=axis_y_limit[0] - 0.5, top=axis_y_limit[1] + 0.5)\n",
    "    ax.set_xlim(axis_x_limit[0], axis_x_limit[1])\n",
    "    ax.set_ylim(axis_y_limit[0], axis_y_limit[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_SVD(embeddings: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Applies SVD dimensionality reduction.\n",
    "\n",
    "    :param embeddings: word embedding matrix of shape (words, dim). In the case\n",
    "                       of a word-word co-occurrence matrix the matrix shape would\n",
    "                       be (words, words).\n",
    "\n",
    "    :return\n",
    "        - 2-dimensional word embedding matrix of shape (words, 2)\n",
    "    \"\"\"\n",
    "    print(\"Running SVD reduction method...\")\n",
    "    svd = TruncatedSVD(n_components=2, n_iter=10)\n",
    "    reduced = svd.fit_transform(embeddings)\n",
    "    print(\"SVD reduction completed!\")\n",
    "\n",
    "    return reduced\n",
    "\n",
    "def reduce_tSNE(embeddings: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Applies t-SNE dimensionality reduction.\n",
    "    \"\"\"\n",
    "    print(\"Running t-SNE reduction method... (it may take a while...)\")\n",
    "    tsne = TSNE(n_components=2, n_iter=1000, metric='cosine')\n",
    "    reduced = tsne.fit_transform(embeddings)\n",
    "    print(\"t-SNE reduction completed!\")\n",
    "\n",
    "    return reduced\n",
    "\n",
    "def reduce_umap(embeddings: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Applies UMAP dimensionality reduction.\n",
    "    \"\"\"\n",
    "    print(\"Running UMAP reduction method... (it may take a while...)\")\n",
    "    umap_emb = umap.UMAP(n_components=2, metric='cosine')\n",
    "    reduced = umap_emb.fit_transform(embeddings)\n",
    "    print(\"UMAP reduction completed!\")\n",
    "    \n",
    "    return reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP\n",
    "reduced_embedding_umap = reduce_umap(embedding_matrix)\n",
    "visualize_embeddings(reduced_embedding_umap, ['whore', 'woman', 'slut', 'girl', 'man', 'boy'], emb_model.key_to_index)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5: Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 6: Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 7: Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 8: Report"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
