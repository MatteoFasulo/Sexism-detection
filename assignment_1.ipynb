{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qmIZOuDv-bQk"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/MatteoFasulo/Sexism-detection/blob/main/assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9vYXSXHf-bQp"
   },
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "rT1NnYib-cZs"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install requests umap nltk gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8HsAwNAc-bQp"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "import re\n",
    "import json\n",
    "from typing import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import umap\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import gensim\n",
    "import gensim.downloader as gloader\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o4MUwVBe-bQt"
   },
   "source": [
    "# Class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "w4OPhRgq-bQu"
   },
   "outputs": [],
   "source": [
    "class SexismDetector:\n",
    "    def __init__(self):\n",
    "\n",
    "        URL_PATTERN_STR = r\"\"\"(?i)((?:https?:(?:/{1,3}|[a-z0-9%])|[a-z0-9.\\-]+[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info\n",
    "                      |int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|\n",
    "                      bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|\n",
    "                      cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|\n",
    "                      gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|\n",
    "                      la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|\n",
    "                      nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|\n",
    "                      sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|\n",
    "                      uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)/)(?:[^\\s()<>{}\\[\\]]+|\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]\n",
    "                      *?\\)|\\([^\\s]+?\\))+(?:\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’])|(?:(?<!@)\n",
    "                      [a-z0-9]+(?:[.\\-][a-z0-9]+)*[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name\n",
    "                      |post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn\n",
    "                      |bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg\n",
    "                      |eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id\n",
    "                      |ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|\n",
    "                      md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|\n",
    "                      ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|\n",
    "                      sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|\n",
    "                      za|zm|zw)\\b/?(?!@)))\"\"\"\n",
    "        self.URL_PATTERN = re.compile(URL_PATTERN_STR, re.IGNORECASE)\n",
    "        self.HASHTAG_PATTERN = re.compile(r'#\\w*')\n",
    "        self.MENTION_PATTERN = re.compile(r'@\\w*')\n",
    "        self.EMOJIS_PATTERN = re.compile(u'([\\U00002600-\\U000027BF])|([\\U0001f300-\\U0001f64F])|([\\U0001f680-\\U0001f6FF])')\n",
    "        self.SPECIAL_CHARACTERS_PATTERN = re.compile(r'&lt;/?[a-z]+&gt;')\n",
    "        self.AND_PATTERN = re.compile(r'&amp;')\n",
    "        self.WORD_PATTERN = re.compile(r'[^a-zA-Z\\s]')\n",
    "        self.SEED = 42\n",
    "        self.DATA_FOLDER = Path('data')\n",
    "        self.columns_to_maintain = ['id_EXIST', 'lang', 'tweet', 'hard_label_task1']\n",
    "        self.UNK_TOKEN = '[UNK]'\n",
    "        self.PAD_TOKEN = '[PAD]'\n",
    "\n",
    "    def download_corpus(self, url: str, filename: str):\n",
    "        if not self.DATA_FOLDER.exists():\n",
    "            self.DATA_FOLDER.mkdir(parents=True)\n",
    "            print(f\"Created folder {self.DATA_FOLDER}.\")\n",
    "\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        with open(self.DATA_FOLDER / filename, 'w', encoding='utf-8') as f:\n",
    "            f.write(response.text)\n",
    "\n",
    "    def load_corpus(self, filename: str, *args, **kwargs):\n",
    "        return pd.read_json(self.DATA_FOLDER / filename, *args, **kwargs)\n",
    "\n",
    "    @staticmethod\n",
    "    def majority_voting(votes: list[str]) -> str:\n",
    "        total_num_votes = len(votes)\n",
    "        yes_votes = votes.count(\"YES\")\n",
    "        no_votes = total_num_votes - yes_votes\n",
    "\n",
    "        if yes_votes > no_votes:\n",
    "            return \"YES\"\n",
    "        elif no_votes > yes_votes:\n",
    "            return \"NO\"\n",
    "        else:\n",
    "            return \"NEUTRAL\" # This will be the case when there is a tie (removed later)\n",
    "\n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        text = self.URL_PATTERN.sub('', text)\n",
    "        text = self.MENTION_PATTERN.sub('', text)\n",
    "        text = self.HASHTAG_PATTERN.sub('', text)\n",
    "        text = self.EMOJIS_PATTERN.sub('', text)\n",
    "        text = self.SPECIAL_CHARACTERS_PATTERN.sub('', text)\n",
    "        text = self.AND_PATTERN.sub('and', text)\n",
    "        text = text.strip()\n",
    "        text = self.WORD_PATTERN.sub(' ', text)\n",
    "        text = text.strip()\n",
    "        return text\n",
    "\n",
    "    def lemmatize_text(self, text: str) -> str:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        downloaded = False\n",
    "        while not downloaded:\n",
    "            try:\n",
    "                lemmatizer.lemmatize(text)\n",
    "                downloaded = True\n",
    "            except LookupError:\n",
    "                print(\"Downloading WordNet...\")\n",
    "                nltk.download('wordnet')\n",
    "        return ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "    @staticmethod\n",
    "    def text_diff(original_text: str, preprocessed_text: str, random: bool = True):\n",
    "        if random:\n",
    "            idx = np.random.randint(0, preprocessed_text.shape[0])\n",
    "        else:\n",
    "            idx = 0\n",
    "\n",
    "        print(f\"Original tweet:\\n{original_text['tweet'].iloc[idx]}\")\n",
    "        print(f\"Processed tweet:\\n{preprocessed_text['tweet'].iloc[idx]}\")\n",
    "\n",
    "    def load_glove(self, model_name: str = 'glove-wiki-gigaword', embedding_dim: int = 50):\n",
    "        self.EMBEDDING_DIM = embedding_dim\n",
    "        return gloader.load(f\"{model_name}-{embedding_dim}\")\n",
    "\n",
    "    def get_vocab(self, data: pd.DataFrame, word_listing: list = None) -> OrderedDict:\n",
    "        idx_to_word = OrderedDict()\n",
    "        word_to_idx = OrderedDict()\n",
    "\n",
    "        tokenizer = nltk.tokenize.NLTKWordTokenizer()\n",
    "\n",
    "        if word_listing is None:\n",
    "            curr_idx = 0\n",
    "            for sentence in data['tweet'].values:\n",
    "                tokens = tokenizer.tokenize(sentence)\n",
    "                for token in tokens:\n",
    "                    if token not in word_to_idx:\n",
    "                        word_to_idx[token] = curr_idx\n",
    "                        idx_to_word[curr_idx] = token\n",
    "                        curr_idx += 1\n",
    "\n",
    "        else:\n",
    "            word_to_idx[self.UNK_TOKEN] = 0\n",
    "            idx_to_word[0] = self.UNK_TOKEN\n",
    "\n",
    "            curr_idx = 1\n",
    "            for sentence in data['tweet'].values:\n",
    "                tokens = sentence.split()\n",
    "                for token in word_listing:\n",
    "                    if token not in word_to_idx:\n",
    "                        word_to_idx[token] = curr_idx\n",
    "                        idx_to_word[curr_idx] = token\n",
    "                        curr_idx += 1\n",
    "\n",
    "        return idx_to_word, word_to_idx\n",
    "\n",
    "    def get_augmented_vocab(self, emb_model: gensim.models.keyedvectors.KeyedVectors, train_words: list, save: bool = False) -> gensim.models.keyedvectors.KeyedVectors:\n",
    "        embedding_vocab = set(emb_model.key_to_index.keys())\n",
    "\n",
    "        new_tokens = []\n",
    "        new_vectors = []\n",
    "        for token in train_words:\n",
    "            #if token not in embedding_vocab:\n",
    "            #    embedding_vocab.add(token)\n",
    "            try:\n",
    "                embedding_vec = emb_model.get_vector(token)\n",
    "            except (KeyError, ValueError):\n",
    "                embedding_vec = ((np.random.rand(self.EMBEDDING_DIM) * 2.0) - 1.0)\n",
    "\n",
    "            new_tokens.append(token)\n",
    "            new_vectors.append(embedding_vec)\n",
    "\n",
    "        emb_model.add_vectors(new_tokens, new_vectors)\n",
    "\n",
    "        if save:\n",
    "            vocab_path = self.DATA_FOLDER / 'vocab2.json'\n",
    "            print(f\"Saving vocab to {vocab_path}\")\n",
    "            with vocab_path.open('w', encoding='utf-8') as f:\n",
    "                json.dump(emb_model.key_to_index, f, indent=4)\n",
    "            print(\"Vocab saved!\")\n",
    "\n",
    "        return emb_model\n",
    "\n",
    "    def get_oov_stats(self, embedding_model: gensim.models.keyedvectors.KeyedVectors, word_listing: list) -> None:\n",
    "        OOV_count = set(word_listing).difference(set(embedding_model.key_to_index.keys()))\n",
    "        OOV_percentage = float(len(OOV_count)) * 100 / len(word_listing)\n",
    "\n",
    "        print(f\"Total OOV terms: {len(OOV_count)} ({OOV_percentage:.2f}%)\")\n",
    "\n",
    "    @staticmethod\n",
    "    def visualize_embeddings(embeddings: np.ndarray,\n",
    "                         word_annotations,\n",
    "                         word_to_idx):\n",
    "        \"\"\"\n",
    "        Plots given reduce word embeddings (2D). Users can highlight specific words (word_annotations list).\n",
    "\n",
    "        :param embeddings: word embedding matrix of shape (words, 2) retrieved via a dimensionality reduction technique.\n",
    "        :param word_annotations: list of words to be annotated.\n",
    "        :param word_to_idx: vocabulary map (word -> index) (dict)\n",
    "        \"\"\"\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(15, 12))\n",
    "\n",
    "        if word_annotations:\n",
    "            print(f\"Annotating words: {word_annotations}\")\n",
    "\n",
    "            word_indexes = []\n",
    "            for word in word_annotations:\n",
    "                word_index = word_to_idx[word]\n",
    "                word_indexes.append(word_index)\n",
    "\n",
    "            word_indexes = np.array(word_indexes)\n",
    "\n",
    "            other_embeddings = embeddings[np.setdiff1d(np.arange(embeddings.shape[0]), word_indexes)]\n",
    "            target_embeddings = embeddings[word_indexes]\n",
    "\n",
    "            ax.scatter(other_embeddings[:, 0], other_embeddings[:, 1], alpha=0.1, c='blue')\n",
    "            ax.scatter(target_embeddings[:, 0], target_embeddings[:, 1], alpha=1.0, c='red')\n",
    "            ax.scatter(target_embeddings[:, 0], target_embeddings[:, 1], alpha=1, facecolors='none', edgecolors='r', s=1000)\n",
    "\n",
    "            for word, word_index in zip(word_annotations, word_indexes):\n",
    "                word_x, word_y = embeddings[word_index, 0], embeddings[word_index, 1]\n",
    "                ax.annotate(word, xy=(word_x, word_y))\n",
    "        else:\n",
    "            ax.scatter(embeddings[:, 0], embeddings[:, 1], alpha=0.1, c='blue')\n",
    "\n",
    "        # We avoid outliers ruining the visualization if they are quite far away\n",
    "        axis_x_limit = (np.min(embeddings[:, 0]), np.max(embeddings[:, 0]))\n",
    "        axis_y_limit = (np.min(embeddings[:, 1]), np.max(embeddings[:, 1]))\n",
    "        plt.xlim(left=axis_x_limit[0] - 0.5, right=axis_x_limit[1] + 0.5)\n",
    "        plt.ylim(bottom=axis_y_limit[0] - 0.5, top=axis_y_limit[1] + 0.5)\n",
    "        ax.set_xlim(axis_x_limit[0], axis_x_limit[1])\n",
    "        ax.set_ylim(axis_y_limit[0], axis_y_limit[1])\n",
    "\n",
    "    @staticmethod\n",
    "    def reduce_SVD(embeddings: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Applies SVD dimensionality reduction.\n",
    "\n",
    "        :param embeddings: word embedding matrix of shape (words, dim). In the case\n",
    "                        of a word-word co-occurrence matrix the matrix shape would\n",
    "                        be (words, words).\n",
    "\n",
    "        :return\n",
    "            - 2-dimensional word embedding matrix of shape (words, 2)\n",
    "        \"\"\"\n",
    "        print(\"Running SVD reduction method...\")\n",
    "        svd = TruncatedSVD(n_components=2, n_iter=10)\n",
    "        reduced = svd.fit_transform(embeddings)\n",
    "        print(\"SVD reduction completed!\")\n",
    "\n",
    "        return reduced\n",
    "\n",
    "    @staticmethod\n",
    "    def reduce_tSNE(embeddings: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Applies t-SNE dimensionality reduction.\n",
    "        \"\"\"\n",
    "        print(\"Running t-SNE reduction method... (it may take a while...)\")\n",
    "        tsne = TSNE(n_components=2, n_iter=1000, metric='cosine')\n",
    "        reduced = tsne.fit_transform(embeddings)\n",
    "        print(\"t-SNE reduction completed!\")\n",
    "\n",
    "        return reduced\n",
    "\n",
    "    @staticmethod\n",
    "    def reduce_umap(embeddings: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Applies UMAP dimensionality reduction.\n",
    "        \"\"\"\n",
    "        print(\"Running UMAP reduction method... (it may take a while...)\")\n",
    "        umap_emb = umap.UMAP(n_components=2, metric='cosine')\n",
    "        reduced = umap_emb.fit_transform(embeddings)\n",
    "        print(\"UMAP reduction completed!\")\n",
    "\n",
    "        return reduced\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9S8JHdj0-bQw"
   },
   "source": [
    "# Task 1: Corpus\n",
    "\n",
    "1. Download the data\n",
    "2. Load the JSON files and encode them as a DataFrame\n",
    "3. Generate hard labels for Task 1 with majority voting\n",
    "4. Filter the DataFrame for only english tweets\n",
    "5. Remove unwanted columns\n",
    "6. Encode the hard labels column as integers\n",
    "\n",
    ">**Bonus**: explore also Spanish tweets leveraging multi-language models and assessing the performance of the model on the two languages in comparison to the English-only model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UX6zDKbU-bQx"
   },
   "outputs": [],
   "source": [
    "detector = SexismDetector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2kB8zolx-bQx"
   },
   "source": [
    "### Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "zHCqDJCq-bQy"
   },
   "outputs": [],
   "source": [
    "detector.download_corpus(url='https://raw.githubusercontent.com/nlp-unibo/nlp-course-material/refs/heads/main/2024-2025/Assignment%201/data/training.json', filename='training.json')\n",
    "detector.download_corpus(url='https://raw.githubusercontent.com/nlp-unibo/nlp-course-material/refs/heads/main/2024-2025/Assignment%201/data/test.json', filename='test.json')\n",
    "detector.download_corpus(url='https://raw.githubusercontent.com/nlp-unibo/nlp-course-material/refs/heads/main/2024-2025/Assignment%201/data/validation.json', filename='validation.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OlbBn1uh-bQy"
   },
   "source": [
    "### Load the JSON files and encode them as a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "xffrWnDg-bQz"
   },
   "outputs": [],
   "source": [
    "train = detector.load_corpus('training.json', orient='index', encoding='utf-8')\n",
    "test = detector.load_corpus('test.json', orient='index', encoding='utf-8')\n",
    "val = detector.load_corpus('validation.json', orient='index', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "id": "R-Iddy7j-bQ0",
    "outputId": "98626d55-a01d-4474-d8f6-b15486c6f0d2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_EXIST</th>\n",
       "      <th>lang</th>\n",
       "      <th>tweet</th>\n",
       "      <th>number_annotators</th>\n",
       "      <th>annotators</th>\n",
       "      <th>gender_annotators</th>\n",
       "      <th>age_annotators</th>\n",
       "      <th>labels_task1</th>\n",
       "      <th>labels_task2</th>\n",
       "      <th>labels_task3</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100001</th>\n",
       "      <td>100001</td>\n",
       "      <td>es</td>\n",
       "      <td>@TheChiflis Ignora al otro, es un capullo.El p...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_1, Annotator_2, Annotator_3, Annota...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 46+, 23-45, 18-22]</td>\n",
       "      <td>[YES, YES, NO, YES, YES, YES]</td>\n",
       "      <td>[REPORTED, JUDGEMENTAL, -, REPORTED, JUDGEMENT...</td>\n",
       "      <td>[[OBJECTIFICATION], [OBJECTIFICATION, SEXUAL-V...</td>\n",
       "      <td>TRAIN_ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100002</th>\n",
       "      <td>100002</td>\n",
       "      <td>es</td>\n",
       "      <td>@ultimonomada_ Si comicsgate se parece en algo...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_7, Annotator_8, Annotator_9, Annota...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 46+, 23-45, 18-22]</td>\n",
       "      <td>[NO, NO, NO, NO, YES, NO]</td>\n",
       "      <td>[-, -, -, -, DIRECT, -]</td>\n",
       "      <td>[[-], [-], [-], [-], [OBJECTIFICATION], [-]]</td>\n",
       "      <td>TRAIN_ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100003</th>\n",
       "      <td>100003</td>\n",
       "      <td>es</td>\n",
       "      <td>@Steven2897 Lee sobre Gamergate, y como eso ha...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_7, Annotator_8, Annotator_9, Annota...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 46+, 23-45, 18-22]</td>\n",
       "      <td>[NO, NO, NO, NO, NO, NO]</td>\n",
       "      <td>[-, -, -, -, -, -]</td>\n",
       "      <td>[[-], [-], [-], [-], [-], [-]]</td>\n",
       "      <td>TRAIN_ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100004</th>\n",
       "      <td>100004</td>\n",
       "      <td>es</td>\n",
       "      <td>@Lunariita7 Un retraso social bastante lamenta...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_13, Annotator_14, Annotator_15, Ann...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 46+, 23-45, 18-22]</td>\n",
       "      <td>[NO, NO, YES, NO, YES, YES]</td>\n",
       "      <td>[-, -, DIRECT, -, REPORTED, REPORTED]</td>\n",
       "      <td>[[-], [-], [IDEOLOGICAL-INEQUALITY], [-], [IDE...</td>\n",
       "      <td>TRAIN_ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100005</th>\n",
       "      <td>100005</td>\n",
       "      <td>es</td>\n",
       "      <td>@novadragon21 @icep4ck @TvDannyZ Entonces como...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_19, Annotator_20, Annotator_21, Ann...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 46+, 23-45, 18-22]</td>\n",
       "      <td>[YES, NO, YES, NO, YES, YES]</td>\n",
       "      <td>[REPORTED, -, JUDGEMENTAL, -, JUDGEMENTAL, DIR...</td>\n",
       "      <td>[[STEREOTYPING-DOMINANCE, OBJECTIFICATION], [-...</td>\n",
       "      <td>TRAIN_ES</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id_EXIST lang                                              tweet  \\\n",
       "100001    100001   es  @TheChiflis Ignora al otro, es un capullo.El p...   \n",
       "100002    100002   es  @ultimonomada_ Si comicsgate se parece en algo...   \n",
       "100003    100003   es  @Steven2897 Lee sobre Gamergate, y como eso ha...   \n",
       "100004    100004   es  @Lunariita7 Un retraso social bastante lamenta...   \n",
       "100005    100005   es  @novadragon21 @icep4ck @TvDannyZ Entonces como...   \n",
       "\n",
       "        number_annotators                                         annotators  \\\n",
       "100001                  6  [Annotator_1, Annotator_2, Annotator_3, Annota...   \n",
       "100002                  6  [Annotator_7, Annotator_8, Annotator_9, Annota...   \n",
       "100003                  6  [Annotator_7, Annotator_8, Annotator_9, Annota...   \n",
       "100004                  6  [Annotator_13, Annotator_14, Annotator_15, Ann...   \n",
       "100005                  6  [Annotator_19, Annotator_20, Annotator_21, Ann...   \n",
       "\n",
       "         gender_annotators                          age_annotators  \\\n",
       "100001  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
       "100002  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
       "100003  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
       "100004  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
       "100005  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
       "\n",
       "                         labels_task1  \\\n",
       "100001  [YES, YES, NO, YES, YES, YES]   \n",
       "100002      [NO, NO, NO, NO, YES, NO]   \n",
       "100003       [NO, NO, NO, NO, NO, NO]   \n",
       "100004    [NO, NO, YES, NO, YES, YES]   \n",
       "100005   [YES, NO, YES, NO, YES, YES]   \n",
       "\n",
       "                                             labels_task2  \\\n",
       "100001  [REPORTED, JUDGEMENTAL, -, REPORTED, JUDGEMENT...   \n",
       "100002                            [-, -, -, -, DIRECT, -]   \n",
       "100003                                 [-, -, -, -, -, -]   \n",
       "100004              [-, -, DIRECT, -, REPORTED, REPORTED]   \n",
       "100005  [REPORTED, -, JUDGEMENTAL, -, JUDGEMENTAL, DIR...   \n",
       "\n",
       "                                             labels_task3     split  \n",
       "100001  [[OBJECTIFICATION], [OBJECTIFICATION, SEXUAL-V...  TRAIN_ES  \n",
       "100002       [[-], [-], [-], [-], [OBJECTIFICATION], [-]]  TRAIN_ES  \n",
       "100003                     [[-], [-], [-], [-], [-], [-]]  TRAIN_ES  \n",
       "100004  [[-], [-], [IDEOLOGICAL-INEQUALITY], [-], [IDE...  TRAIN_ES  \n",
       "100005  [[STEREOTYPING-DOMINANCE, OBJECTIFICATION], [-...  TRAIN_ES  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3PwyCeQc-bQ1",
    "outputId": "e32f4d75-d4ae-4df0-c22b-c04092ee102e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6920, 11), (726, 11), (312, 11))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, val.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4fISdCga-bQ1"
   },
   "source": [
    "### Generate hard labels for Task 1 with majority voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "jVfdt1mf-bQ1"
   },
   "outputs": [],
   "source": [
    "train['hard_label_task1'] = train['labels_task1'].apply(detector.majority_voting)\n",
    "val['hard_label_task1'] = val['labels_task1'].apply(detector.majority_voting)\n",
    "test['hard_label_task1'] = test['labels_task1'].apply(detector.majority_voting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iNSfxEqA-bQ2"
   },
   "source": [
    "### Filter the DataFrame for only english tweets and remove unclear tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "P3hE1ua_-bQ2"
   },
   "outputs": [],
   "source": [
    "train = train[(train['hard_label_task1'] != \"NEUTRAL\") & (train['lang'] == \"en\")]\n",
    "val = val[(val['hard_label_task1'] != \"NEUTRAL\") & (val['lang'] == \"en\")]\n",
    "test = test[(test['hard_label_task1'] != \"NEUTRAL\") & (test['lang'] == \"en\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "js6SiYSY-bQ2",
    "outputId": "67b99dfa-633f-47d1-bf92-bcc6f2215d93"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2870, 12), (158, 12), (286, 12))"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, val.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "edgN4FWb-bQ2"
   },
   "source": [
    "### Remove unwanted columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "lFUcf_Ii-bQ3"
   },
   "outputs": [],
   "source": [
    "train = train[detector.columns_to_maintain]\n",
    "val = val[detector.columns_to_maintain]\n",
    "test = test[detector.columns_to_maintain]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "BEe_i8hJ-bQ3",
    "outputId": "f06531fd-1aaa-430f-a1d2-8e81f546707b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_EXIST</th>\n",
       "      <th>lang</th>\n",
       "      <th>tweet</th>\n",
       "      <th>hard_label_task1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200002</th>\n",
       "      <td>200002</td>\n",
       "      <td>en</td>\n",
       "      <td>Writing a uni essay in my local pub with a cof...</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200003</th>\n",
       "      <td>200003</td>\n",
       "      <td>en</td>\n",
       "      <td>@UniversalORL it is 2021 not 1921. I dont appr...</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200006</th>\n",
       "      <td>200006</td>\n",
       "      <td>en</td>\n",
       "      <td>According to a customer I have plenty of time ...</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200007</th>\n",
       "      <td>200007</td>\n",
       "      <td>en</td>\n",
       "      <td>So only 'blokes' drink beer? Sorry, but if you...</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200008</th>\n",
       "      <td>200008</td>\n",
       "      <td>en</td>\n",
       "      <td>New to the shelves this week - looking forward...</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id_EXIST lang                                              tweet  \\\n",
       "200002    200002   en  Writing a uni essay in my local pub with a cof...   \n",
       "200003    200003   en  @UniversalORL it is 2021 not 1921. I dont appr...   \n",
       "200006    200006   en  According to a customer I have plenty of time ...   \n",
       "200007    200007   en  So only 'blokes' drink beer? Sorry, but if you...   \n",
       "200008    200008   en  New to the shelves this week - looking forward...   \n",
       "\n",
       "       hard_label_task1  \n",
       "200002              YES  \n",
       "200003              YES  \n",
       "200006              YES  \n",
       "200007              YES  \n",
       "200008               NO  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C1vO0qNe-bQ3"
   },
   "source": [
    "### Encode the hard labels column as integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "_vKw9ugP-bQ3"
   },
   "outputs": [],
   "source": [
    "train['hard_label_task1'] = train['hard_label_task1'].apply(lambda x: 1 if x == \"YES\" else 0)\n",
    "val['hard_label_task1'] = val['hard_label_task1'].apply(lambda x: 1 if x == \"YES\" else 0)\n",
    "test['hard_label_task1'] = test['hard_label_task1'].apply(lambda x: 1 if x == \"YES\" else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "id": "QYpFYiwx-bQ4",
    "outputId": "eb4f3686-ace7-40f8-90cf-1926603dc94f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hard_label_task1\n",
       "0    1733\n",
       "1    1137\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.hard_label_task1.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D_qchDt7-bQ4"
   },
   "source": [
    "# Task 2: Data Cleaning\n",
    "\n",
    "1. Remove emojis\n",
    "2. Remove hashtags (e.g. #metoo)\n",
    "3. Remove mentions (e.g. @user)\n",
    "4. Remove URLs\n",
    "5. Remove special characters and symbols\n",
    "6. Remove specific quote characters (e.g. curly quotes)\n",
    "7. Perform lemmatization\n",
    "\n",
    ">**Bonus**: use other preprocessing strategies exploring techniques tailored specifically for tweets or methods that are common in social media text processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CPZ_A3ek-bQ4"
   },
   "source": [
    "The way to go (priority order) is the following:\n",
    "1. Remove URLs\n",
    "2. Remove mentions\n",
    "3. Remove hashtags\n",
    "4. Remove emojis\n",
    "5. Remove special characters\n",
    "6. Remove specific quote characters\n",
    "7. Perform lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "vyKl2917-bQ4"
   },
   "outputs": [],
   "source": [
    "original_train = train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "xtI27hJo-bQ4"
   },
   "outputs": [],
   "source": [
    "train['tweet'] = train['tweet'].apply(detector.preprocess_text)\n",
    "val['tweet'] = val['tweet'].apply(detector.preprocess_text)\n",
    "test['tweet'] = test['tweet'].apply(detector.preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yFOqFd6i-bQ6"
   },
   "source": [
    "### Perform lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "lu1toksv-bQ6"
   },
   "outputs": [],
   "source": [
    "train['tweet'] = train['tweet'].apply(detector.lemmatize_text)\n",
    "val['tweet'] = val['tweet'].apply(detector.lemmatize_text)\n",
    "test['tweet'] = test['tweet'].apply(detector.lemmatize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8R-fL1sl-bQ7"
   },
   "source": [
    "### Cast text to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "Ku484oN9-bQ7"
   },
   "outputs": [],
   "source": [
    "train['tweet'] = train['tweet'].str.lower()\n",
    "val['tweet'] = val['tweet'].str.lower()\n",
    "test['tweet'] = test['tweet'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dFbPle-9-bQ8"
   },
   "source": [
    "### Show the difference between the original and cleaned text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TURQi4uE-bQ8",
    "outputId": "6b6268ed-87e4-41ed-8bd9-af1dd77d51c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tweet:\n",
      "@nick__nobody @realDeanCool still pumpin’ out the misinformation for the boys’ club\n",
      "Processed tweet:\n",
      "still pumpin out the misinformation for the boy club\n"
     ]
    }
   ],
   "source": [
    "detector.text_diff(preprocessed_text=train, original_text=original_train, random=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "50Cj7hDE-bQ9"
   },
   "source": [
    "# Task 3: Text Encoding\n",
    "\n",
    "* Embed words using GloVe embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e3m8P2pe-bQ-"
   },
   "source": [
    "### Embed words using GloVe embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vEGg4Jvp-bQ-",
    "outputId": "a06a7677-a807-4a62-8792-f4b6242f7adf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_model = detector.load_glove(model_name='glove-wiki-gigaword', embedding_dim=detector.EMBEDDING_DIM)\n",
    "len(emb_model.key_to_index.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "JJEQVEKJLcKF"
   },
   "outputs": [],
   "source": [
    "train_idx_to_word, train_word_to_idx = detector.get_vocab(train)\n",
    "train_word_listing = list(train_idx_to_word.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total OOV terms: 868 (8.81%)\n"
     ]
    }
   ],
   "source": [
    "detector.get_oov_stats(emb_model, train_word_listing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fnNshkRb-bRF",
    "outputId": "cba4801b-47e4-4188-bd8e-134d2e30d6d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving vocab to data\\vocab2.json\n",
      "Vocab saved!\n"
     ]
    }
   ],
   "source": [
    "emb_model_augmented = detector.get_augmented_vocab(emb_model, train_words=train_word_listing, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "rMMto8pR-bRF"
   },
   "outputs": [],
   "source": [
    "# add the UNK token to the embedding model with the vector which is the average of all the vectors\n",
    "emb_model_augmented.add_vectors([\"[UNK]\", \"[PAD]\"], [np.mean(emb_model.vectors, axis=0), np.zeros(detector.EMBEDDING_DIM)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SVYSXzcL-bRF",
    "outputId": "c1db1bab-0267-4024-b20f-11cc86003eb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total OOV terms: 0 (0.00%)\n",
      "Total OOV terms: 0 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "#detector.get_oov_stats(emb_model_augmented, val_word_listing)\n",
    "#detector.get_oov_stats(emb_model_augmented, test_word_listing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yRw9DQEq-bRG",
    "outputId": "1c1fc2bb-7b70-4885-bc7e-2d9960853f1e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400870, 50)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Create the embeddings matrix\n",
    "embedding_matrix = emb_model.vectors\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "FZvQAyL_-bRG"
   },
   "outputs": [],
   "source": [
    "# UMAP\n",
    "#reduced_embedding_umap = detector.reduce_umap(embedding_matrix)\n",
    "#detector.visualize_embeddings(reduced_embedding_umap, ['whore', 'woman', 'slut', 'girl', 'man', 'boy'], emb_model.key_to_index)\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0UUVw8lx-bRG"
   },
   "source": [
    "# Task 4: Model definition\n",
    "\n",
    "* Baseline: Implement a Bidirectional LSTM with a Dense layer on top.\n",
    "* Model 1: add an additional LSTM layer to the baseline model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xpU6TmVd-bRG"
   },
   "source": [
    "### Baseline: Implement a Bidirectional LSTM with a Dense layer on top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineLSTM(torch.nn.Module):\n",
    "    def __init__(self, embedding_model, ):\n",
    "        super(BaselineLSTM, self).__init__()\n",
    "\n",
    "        self.word_embeddings = torch.nn.Embedding.from_pretrained(\n",
    "            embeddings=torch.from_numpy(embedding_model.vectors),\n",
    "            freeze=False,\n",
    "            padding_idx=embedding_model.get_index(detector.PAD_TOKEN),\n",
    "        )\n",
    "        self.lstm = torch.nn.LSTM(input_size=embedding_model.vector_size, hidden_size=128, bidirectional=True, batch_first=True)\n",
    "        self.dense = torch.nn.Linear(in_features=256, out_features=1)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        embeds = self.word_embeddings(inputs)\n",
    "        h0 = torch.zeros(2, embeds.size(0), 128).to(embeds.device)\n",
    "        c0 = torch.zeros(2, embeds.size(0), 128).to(embeds.device)\n",
    "        lstm_out, _ = self.lstm(embeds, (h0, c0))\n",
    "        lstm_out = torch.nn.functional.tanh(lstm_out)\n",
    "        output = self.dense(lstm_out[:, -1, :])\n",
    "        return output\n",
    "\n",
    "model = BaselineLSTM(emb_model_augmented).to(device)\n",
    "loss_function = torch.nn.BCEWithLogitsLoss(reduction='none')\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_padded_sequences(data: pd.Series, embedding_model):\n",
    "    tokenizer = nltk.tokenize.NLTKWordTokenizer()\n",
    "    unk_index = embedding_model.get_index(detector.UNK_TOKEN)\n",
    "    int_sequences = [torch.tensor([embedding_model.get_index(word) if word in embedding_model.key_to_index else unk_index for word in tokenizer.tokenize(x)]) for x in data.values]\n",
    "    #MAX_SEQ_LENGTH = max([len(i) for i in int_sequences])\n",
    "    return torch.nn.utils.rnn.pad_sequence(int_sequences, padding_value=embedding_model.get_index(detector.PAD_TOKEN), batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.sequences.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.labels[idx]\n",
    "\n",
    "sequences = get_padded_sequences(train['tweet'], emb_model_augmented)\n",
    "labels = torch.tensor(train['hard_label_task1'].values)\n",
    "\n",
    "training_data = torch.utils.data.DataLoader(\n",
    "    dataset=TextDataset(sequences, labels),\n",
    "    batch_size=32,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "validation_data = torch.utils.data.DataLoader(\n",
    "    dataset=TextDataset(get_padded_sequences(val['tweet'], emb_model_augmented), torch.tensor(val['hard_label_task1'].values)),\n",
    "    batch_size=32,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Loss: 0.2405 - Val Loss: 0.0247 - Val Acc: 0.7722\n",
      "Epoch 2/20 - Loss: 0.0249 - Val Loss: 0.0222 - Val Acc: 0.7911\n",
      "Epoch 3/20 - Loss: 0.3804 - Val Loss: 0.0194 - Val Acc: 0.8291\n",
      "Epoch 4/20 - Loss: 0.0483 - Val Loss: 0.0186 - Val Acc: 0.8291\n",
      "Epoch 5/20 - Loss: 0.0167 - Val Loss: 0.0201 - Val Acc: 0.8228\n",
      "Epoch 6/20 - Loss: 0.0353 - Val Loss: 0.0236 - Val Acc: 0.7785\n",
      "Epoch 7/20 - Loss: 0.0154 - Val Loss: 0.0194 - Val Acc: 0.8165\n",
      "Epoch 8/20 - Loss: 0.0122 - Val Loss: 0.0212 - Val Acc: 0.8291\n",
      "Epoch 9/20 - Loss: 0.0133 - Val Loss: 0.0206 - Val Acc: 0.8418\n",
      "Epoch 10/20 - Loss: 0.0126 - Val Loss: 0.0213 - Val Acc: 0.8038\n",
      "Epoch 11/20 - Loss: 0.0103 - Val Loss: 0.0234 - Val Acc: 0.8101\n",
      "Epoch 12/20 - Loss: 0.0118 - Val Loss: 0.0238 - Val Acc: 0.8101\n",
      "Epoch 13/20 - Loss: 0.2145 - Val Loss: 0.0209 - Val Acc: 0.8228\n",
      "Epoch 14/20 - Loss: 0.0142 - Val Loss: 0.0259 - Val Acc: 0.7848\n",
      "Epoch 15/20 - Loss: 0.0109 - Val Loss: 0.0196 - Val Acc: 0.8228\n",
      "Epoch 16/20 - Loss: 0.0086 - Val Loss: 0.0214 - Val Acc: 0.8481\n",
      "Epoch 17/20 - Loss: 0.0125 - Val Loss: 0.0231 - Val Acc: 0.8101\n",
      "Epoch 18/20 - Loss: 0.0074 - Val Loss: 0.0239 - Val Acc: 0.8165\n",
      "Epoch 19/20 - Loss: 0.0123 - Val Loss: 0.0229 - Val Acc: 0.7975\n",
      "Epoch 20/20 - Loss: 0.0068 - Val Loss: 0.0235 - Val Acc: 0.8291\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for i, batch in enumerate(training_data):\n",
    "        # Get the inputs; data is a list of [inputs, labels]\n",
    "        sentence, labels = batch\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Send the data to the device\n",
    "        sentence = sentence.to(device)\n",
    "        labels = labels.unsqueeze(1).float().to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(sentence)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = loss_function(output, labels)\n",
    "        loss_mask = labels != emb_model_augmented.get_index(detector.PAD_TOKEN)\n",
    "        loss_masked = loss.where(loss_mask, torch.tensor(0.0))\n",
    "        mean_loss = loss_masked.sum() / loss_mask.sum()\n",
    "\n",
    "        # Backward pass\n",
    "        mean_loss.backward()\n",
    "        \n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i, batch in enumerate(validation_data):\n",
    "            sentence, labels = batch\n",
    "\n",
    "            sentence = sentence.to(device)\n",
    "            labels = labels.unsqueeze(1).float().to(device)\n",
    "\n",
    "            output = model(sentence)\n",
    "\n",
    "            val_loss += loss_function(output, labels).sum().item()\n",
    "\n",
    "            predicted = (output > 0.5).float()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Print the loss and accuracy for each epoch for training and validation\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} - Loss: {loss.item():.4f} - Val Loss: {val_loss / total:.4f} - Val Acc: {correct / total:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WGyl3DEr-bRK"
   },
   "source": [
    "# Task 5: Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_849c_EG-bRK"
   },
   "source": [
    "# Task 6: Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qoCoBkHN-bRK"
   },
   "source": [
    "# Task 7: Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lCNhVqZ9-bRL"
   },
   "source": [
    "# Task 8: Report"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
