{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WeCeITXoxLf"
      },
      "source": [
        "# Assignment 2\n",
        "\n",
        "## Group\n",
        "\n",
        "* Luca Babboni - luca.babboni2@studio.unibo.it\n",
        "* Matteo Fasulo - matteo.fasulo@studio.unibo.it\n",
        "* Maksim Omelchenko - maksim.omelchenko@studio.unibo.it\n",
        "* Luca Tedeschini - luca.tedeschini3@studio.unibo.it\n",
        "\n",
        "## Description\n",
        "\n",
        "This notebook addresses [EDOS Task A](https://github.com/rewire-online/edos) on sexism detection.\n",
        "\n",
        "## Problem definition\n",
        "\n",
        "Given an input text sentence, the task is to label the sentence as sexist or not sexist (binary classification).\n",
        "\n",
        "### Examples:\n",
        "\n",
        "**Text**: *``Schedule a date with her, then don't show up. Then text her \"GOTCHA B___H\".''*\n",
        "\n",
        "**Label**: Sexist\n",
        "\n",
        "**Text**: *``That’s completely ridiculous a woman flashing her boobs is not sexual assault in the slightest.''*\n",
        "\n",
        "**Label**: Not sexist\n",
        "\n",
        "## Approach\n",
        "\n",
        "We will tackle the binary classification task with LLMs.\n",
        "\n",
        "In particular, we'll consider zero-/few-shot prompting approaches to assess the capability of some popular open-source LLMs on this task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import requests\n",
        "import os\n",
        "from copy import deepcopy\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_uWEUjs0THxP",
        "outputId": "58ebfc54-3ead-47f9-8ac0-25b336bb6b5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): Traceback (most recent call last):\n",
            "  File \"/home/matte/Sexism-detection/.venv/bin/huggingface-cli\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/home/matte/Sexism-detection/.venv/lib/python3.12/site-packages/huggingface_hub/commands/huggingface_cli.py\", line 57, in main\n",
            "    service.run()\n",
            "  File \"/home/matte/Sexism-detection/.venv/lib/python3.12/site-packages/huggingface_hub/commands/user.py\", line 153, in run\n",
            "    login(\n",
            "  File \"/home/matte/Sexism-detection/.venv/lib/python3.12/site-packages/huggingface_hub/_login.py\", line 123, in login\n",
            "    interpreter_login(new_session=new_session, write_permission=write_permission)\n",
            "  File \"/home/matte/Sexism-detection/.venv/lib/python3.12/site-packages/huggingface_hub/_login.py\", line 275, in interpreter_login\n",
            "    token = getpass(\"Enter your token (input will not be visible): \")\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/getpass.py\", line 77, in unix_getpass\n",
            "    passwd = _raw_input(prompt, stream, input=input)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/getpass.py\", line 146, in _raw_input\n",
            "    line = input.readline()\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "  File \"<frozen codecs>\", line 319, in decode\n",
            "KeyboardInterrupt\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qg6pcnGYRQKP"
      },
      "source": [
        "# Download the data\n",
        "\n",
        "We download the data from the repository of the course."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TAnHnh3qQBu3"
      },
      "outputs": [],
      "source": [
        "def download_corpus(url: str, filename: str) -> None:\n",
        "    \"\"\"\n",
        "    Downloads a text corpus from a given URL and saves it to a specified filename within the data folder if not exist\n",
        "\n",
        "    Args:\n",
        "        url (str): The URL from which to download the corpus.\n",
        "        filename (str): The name of the file to save the downloaded corpus.\n",
        "\n",
        "    Raises:\n",
        "        requests.exceptions.HTTPError: If the HTTP request returned an unsuccessful status code.\n",
        "\n",
        "    Side Effects:\n",
        "        Creates the data folder if it does not exist.\n",
        "        Writes the downloaded corpus to the specified file.\n",
        "    \"\"\"\n",
        "    data_folder = Path(\"./data\")\n",
        "    if not data_folder.exists():\n",
        "      data_folder.mkdir(parents=True)\n",
        "      print(f\"Created folder {data_folder}.\")\n",
        "\n",
        "    if not (data_folder / filename).exists():\n",
        "      response = requests.get(url)\n",
        "      response.raise_for_status()\n",
        "      with open(data_folder / filename, 'w', encoding='utf-8') as f:\n",
        "        f.write(response.text)\n",
        "      print(f\"Created file {data_folder / filename}.\")\n",
        "    else:\n",
        "      print(f\"File {data_folder / filename} already exists.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "path_test = 'https://raw.githubusercontent.com/nlp-unibo/nlp-course-material/refs/heads/main/2024-2025/Assignment%202/data/a2_test.csv'\n",
        "path_demonstrations = 'https://raw.githubusercontent.com/nlp-unibo/nlp-course-material/refs/heads/main/2024-2025/Assignment%202/data/demonstrations.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITl0duXPQdUo",
        "outputId": "d1d88256-8d6c-4b03-fc30-5371c03c1b3b"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'Path' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdownload_corpus\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ma2_test.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m download_corpus(path_demonstrations, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdemonstrations.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "Cell \u001b[0;32mIn[1], line 16\u001b[0m, in \u001b[0;36mdownload_corpus\u001b[0;34m(url, filename)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdownload_corpus\u001b[39m(url: \u001b[38;5;28mstr\u001b[39m, filename: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m    Downloads a text corpus from a given URL and saves it to a specified filename within the data folder if not exist\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m        Writes the downloaded corpus to the specified file.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m     data_folder \u001b[38;5;241m=\u001b[39m \u001b[43mPath\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data_folder\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m     18\u001b[0m       data_folder\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Path' is not defined"
          ]
        }
      ],
      "source": [
        "download_corpus(path_test, 'a2_test.csv')\n",
        "download_corpus(path_demonstrations, 'demonstrations.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUDarjlVRWap"
      },
      "source": [
        "# Encode a2_test.csv into a pandas.DataFrame object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "IvEoUw3mRVsy",
        "outputId": "20753409-e686-4e51-a562-8f9fb31cdd76"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rewire_id</th>\n",
              "      <th>text</th>\n",
              "      <th>label_sexist</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>sexism2022_english-17133</td>\n",
              "      <td>The boys must be gaming because there goes the...</td>\n",
              "      <td>not sexist</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>sexism2022_english-14197</td>\n",
              "      <td>Look at those eyes. Either someone unexpectedl...</td>\n",
              "      <td>sexist</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>sexism2022_english-3018</td>\n",
              "      <td>Old man mogs everyone in this sub</td>\n",
              "      <td>not sexist</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>sexism2022_english-5301</td>\n",
              "      <td>Excellent, I was just looking at another post ...</td>\n",
              "      <td>not sexist</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>sexism2022_english-17796</td>\n",
              "      <td>So you run back to daddy whenever you need hel...</td>\n",
              "      <td>sexist</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  rewire_id  \\\n",
              "0  sexism2022_english-17133   \n",
              "1  sexism2022_english-14197   \n",
              "2   sexism2022_english-3018   \n",
              "3   sexism2022_english-5301   \n",
              "4  sexism2022_english-17796   \n",
              "\n",
              "                                                text label_sexist  \n",
              "0  The boys must be gaming because there goes the...   not sexist  \n",
              "1  Look at those eyes. Either someone unexpectedl...       sexist  \n",
              "2                  Old man mogs everyone in this sub   not sexist  \n",
              "3  Excellent, I was just looking at another post ...   not sexist  \n",
              "4  So you run back to daddy whenever you need hel...       sexist  "
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv('./data/a2_test.csv', encoding='utf-8')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJp08l4yLJnd"
      },
      "source": [
        "# [Task 1 - 0.5 points] Model setup\n",
        "\n",
        "Once the test data has been loaded, we have to setup the model pipeline for inference.\n",
        "\n",
        "In particular, we have to:\n",
        "- Load the model weights from Huggingface\n",
        "- Quantize the model to fit into a single-GPU limited hardware"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptffotFIjq89"
      },
      "source": [
        "## Which LLMs?\n",
        "\n",
        "The pool of LLMs is ever increasing and it's impossible to keep track of all new entries.\n",
        "\n",
        "We focus on popular open-source models.\n",
        "\n",
        "- [Mistral v2](mistralai/Mistral-7B-Instruct-v0.2)\n",
        "- [Mistral v3](mistralai/Mistral-7B-Instruct-v0.3)\n",
        "- [Llama v3.1](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct)\n",
        "- [Phi3-mini](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct)\n",
        "\n",
        "Other open-source models are more than welcome!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uH1YShLfLJnd"
      },
      "source": [
        "### Instructions\n",
        "\n",
        "In order to get Task 1 points, we require you to:\n",
        "\n",
        "* Pick 2 model cards from the provided list.\n",
        "* For each model:\n",
        "  - Define a separate section of your notebook for the model.\n",
        "  - Setup a quantization configuration for the model.\n",
        "  - Load the model via HuggingFace APIs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8ORld7klCfG"
      },
      "source": [
        "### Notes\n",
        "\n",
        "1. There's a popular library integrated with Huggingface's ``transformers`` to perform quantization.\n",
        "\n",
        "2. Define two separate sections of your notebook to show that you have implemented the prompting pipeline for each selected model card."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "mXOrrN4oSHck"
      },
      "outputs": [],
      "source": [
        "def load_model(model_card):\n",
        "  quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_card)\n",
        "  model_8bit = AutoModelForCausalLM.from_pretrained(model_card,\n",
        "                                                    quantization_config=quantization_config)\n",
        "  model_8bit.eval()\n",
        "  return model_8bit, tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s30F-owJTEY7"
      },
      "source": [
        "# Mistral v3 instruct in 8bit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208,
          "referenced_widgets": [
            "01d5928c42974396ab80006ca5356e91",
            "e3caf2ab8e174e5a97b028e6e37c7022",
            "68c1835888db436f926b0bf305c329d3",
            "febe3f2209be43ff820881ac4c639286",
            "e10b34b3ed334316ad575cf813f2ba42",
            "3b11ecfa0f1244e6a9285b7bcf78692c",
            "4950ece0756e4885a3c8cd9f1bb8d606",
            "a5cae6bf80424856ae15f4643cdf7c68",
            "e2a632d31e3b4c438228a3a4769f2b32",
            "e19f61f23acb42cb84b6aebb80526d7a",
            "595a52a1d177493291b4405059c2bba9"
          ]
        },
        "id": "iYR6pRKDWR-u",
        "outputId": "1424170b-6fcb-4549-db35-273d6f50f700"
      },
      "outputs": [],
      "source": [
        "model_card = 'mistralai/Mistral-7B-Instruct-v0.3'\n",
        "model_8bit, tokenizer = load_model(model_card)\n",
        "\n",
        "device = model_8bit.device\n",
        "print(f'Model loaded on {device}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzNNzb1VLJnd"
      },
      "source": [
        "# [Task 2 - 1.0 points] Prompt setup\n",
        "\n",
        "Prompting requires an input pre-processing phase where we convert each input example into a specific instruction prompt.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GaBtKXomY_m"
      },
      "source": [
        "## Prompt Template\n",
        "\n",
        "Use the following prompt template to process input texts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7e8P-Kk8me6q"
      },
      "outputs": [],
      "source": [
        "prompt = [\n",
        "    {\n",
        "        'role': 'system',\n",
        "        'content': 'You are an annotator for sexism detection.'\n",
        "    },\n",
        "    {\n",
        "        'role': 'user',\n",
        "        'content': \"\"\"Your task is to classify input text as containing sexism or not. Respond only YES or NO.\n",
        "\n",
        "        TEXT:\n",
        "        {text}\n",
        "\n",
        "        ANSWER:\n",
        "        \"\"\"\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHeoEN7MLJnd"
      },
      "source": [
        "### Instructions\n",
        "\n",
        "In order to get Task 2 points, we require you to:\n",
        "\n",
        "* Write a ``prepare_prompts`` function as the one reported below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "PUjRVMtMm9CE"
      },
      "outputs": [],
      "source": [
        "def prepare_prompts(texts, prompt_template, tokenizer, device):\n",
        "  \"\"\"\n",
        "    This function format input text samples into instructions prompts.\n",
        "\n",
        "    Inputs:\n",
        "      texts: input texts to classify via prompting\n",
        "      prompt_template: the prompt template provided in this assignment\n",
        "      tokenizer: the transformers Tokenizer object instance associated with the chosen model card\n",
        "\n",
        "    Outputs:\n",
        "      input texts to classify in the form of instruction prompts\n",
        "  \"\"\"\n",
        "  prompts = []\n",
        "  for text in texts:\n",
        "    prompt_with_text = deepcopy(prompt_template)\n",
        "    # add text\n",
        "    prompt_with_text[1]['content'] = prompt_with_text[1]['content'].replace('{text}', text)\n",
        "\n",
        "    prompts.append(tokenizer.apply_chat_template(prompt_with_text, tokenize=True,\n",
        "                                                 add_generation_prompt=True,\n",
        "                                                 return_dict=True,\n",
        "                                                 return_tensors=\"pt\").to(device))\n",
        "\n",
        "  return prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "y8s1z97qdOXp"
      },
      "outputs": [],
      "source": [
        "prompts = prepare_prompts(df['text'], prompt, tokenizer, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxK_wCGreL6t",
        "outputId": "e69dae3f-b52a-4740-f2e1-8b78e4923617"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You are an annotator for sexism detection.\n",
            "\n",
            "Your task is to classify input text as containing sexism or not. Respond only YES or NO.\n",
            "\n",
            "        TEXT:\n",
            "        The boys must be gaming because there goes the wifi.\n",
            "\n",
            "        ANSWER:\n",
            "        \n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.decode(prompts[0].input_ids[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOCRgGf7mifk"
      },
      "source": [
        "### Notes\n",
        "\n",
        "1. You are free to modify the prompt format (**not its content**) as you like depending on your code implementation.\n",
        "\n",
        "2. Note that the provided prompt has placeholders. You need to format the string to replace placeholders. Huggingface might have dedicated APIs for this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgBhkBwuLJnd"
      },
      "source": [
        "# [Task 3 - 1.0 points] Inference\n",
        "\n",
        "We are now ready to define the inference loop where we prompt the model with each pre-processed sample."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WsrQSvcLJnd"
      },
      "source": [
        "### Instructions\n",
        "\n",
        "In order to get Task 3 points, we require you to:\n",
        "\n",
        "* Write a ``generate_responses`` function as the one reported below.\n",
        "* Write a ``process_response`` function as the one reported below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "bG3CDXNlyD5k"
      },
      "outputs": [],
      "source": [
        "def generate_responses(model, prompt_examples):\n",
        "  \"\"\"\n",
        "    This function implements the inference loop for a LLM model.\n",
        "    Given a set of examples, the model is tasked to generate a response.\n",
        "\n",
        "    Inputs:\n",
        "      model: LLM model instance for prompting\n",
        "      prompt_examples: pre-processed text samples\n",
        "\n",
        "    Outputs:\n",
        "      generated responses\n",
        "  \"\"\"\n",
        "  answers = []\n",
        "  for prompt in tqdm(prompt_examples):\n",
        "    response = model.generate(**prompt, max_new_tokens=1000, pad_token_id=tokenizer.eos_token_id)\n",
        "    answers.append(response)\n",
        "  return answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "uiCoMrutyXTU"
      },
      "outputs": [],
      "source": [
        "def process_response(response):\n",
        "  \"\"\"\n",
        "    This function takes a textual response generated by the LLM\n",
        "    and processes it to map the response to a binary label.\n",
        "\n",
        "    Inputs:\n",
        "      response: generated response from LLM\n",
        "\n",
        "    Outputs:\n",
        "      parsed binary response: return 1 if YES and 0 if NO\n",
        "  \"\"\"\n",
        "  response = tokenizer.decode(response[0])\n",
        "  if 'YES' in response.split('ANSWER')[-1]:\n",
        "    return 1\n",
        "  else:\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EeJbZERjd1Lr",
        "outputId": "b64c3a15-d641-43fc-c3ed-551af98bad8b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [04:21<00:00,  1.15it/s]\n"
          ]
        }
      ],
      "source": [
        "answers = generate_responses(model_8bit, prompts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKcQ0ZfO2Fya"
      },
      "source": [
        "## Notes\n",
        "\n",
        "1. According to our tests, it should take you ~10 mins to perform full inference on 300 samples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyZ8WU09zz-a"
      },
      "source": [
        "# [Task 4 - 0.5 points] Metrics\n",
        "\n",
        "In order to evaluate selected LLMs, we need to compute performance metrics.\n",
        "\n",
        "In particular, we are interested in computing **accuracy** since the provided data is balanced with respect to classification classes.\n",
        "\n",
        "Moreover, we want to compute the ratio of failed responses generated by models.\n",
        "\n",
        "That is, how frequent the LLM fails to follow instructions and provides incorrect responses that do not address the classification task.\n",
        "\n",
        "We denote this metric as **fail-ratio**.\n",
        "\n",
        "In summary, we parse generated responses as follows:\n",
        "- 1 if the model says YES\n",
        "- 0 if the model says NO\n",
        "- 0 if the model does not answer in either way"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6lu64o80iX4"
      },
      "source": [
        "### Instructions\n",
        "\n",
        "In order to get Task 4 points, we require you to:\n",
        "\n",
        "* Write a ``compute_metrics`` function as the one reported below.\n",
        "* Compute metrics for the two selected LLMs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "9Fmcw_9v0k9D"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(responses, y_true):\n",
        "  \"\"\"\n",
        "    This function takes predicted and ground-truth labels and compute metrics.\n",
        "    In particular, this function compute accuracy and fail-ratio metrics.\n",
        "    This function internally invokes `process_response` to compute metrics.\n",
        "\n",
        "    Inputs:\n",
        "      responses: generated LLM responses\n",
        "      y_true: ground-truth binary labels\n",
        "\n",
        "    Outputs:\n",
        "      dictionary containing desired metrics\n",
        "  \"\"\"\n",
        "  y_pred = [process_response(response) for response in responses]\n",
        "  accuracy = (np.array(y_pred) == np.array(y_true)).mean()\n",
        "  fail_ratio = (np.array(y_pred) != np.array(y_true)).mean()\n",
        "  return {'accuracy': accuracy, 'fail_ratio': fail_ratio}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AD1o05TklVKd",
        "outputId": "43d629e8-3efe-4e04-e5ae-608963653dae"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'accuracy': 0.6133333333333333, 'fail_ratio': 0.38666666666666666}"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_true = df['label_sexist'] == 'sexist'\n",
        "compute_metrics(answers, x_true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHyvV4QD2vZS"
      },
      "source": [
        "# [Task 5 - 1.0 points] Few-shot Inference\n",
        "\n",
        "So far, we have tested models in a zero-shot fashion: we provide the input text to classify and instruct the model to generate a response.\n",
        "\n",
        "We are now interested in performing few-shot prompting to see the impact of providing demonstration examples.\n",
        "\n",
        "To do so, we slightly change the prompt template as follows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "pEqsOHz63ReW"
      },
      "outputs": [],
      "source": [
        "prompt_few_shot = [\n",
        "    {\n",
        "        'role': 'system',\n",
        "        'content': 'You are an annotator for sexism detection.'\n",
        "    },\n",
        "    {\n",
        "        'role': 'user',\n",
        "        'content': \"\"\"Your task is to classify input text as containing sexism or not. Respond only YES or NO.\n",
        "\n",
        "        EXAMPLES:\n",
        "        {examples}\n",
        "\n",
        "        TEXT:\n",
        "        {text}\n",
        "\n",
        "        ANSWER:\n",
        "        \"\"\"\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SnaxuwN3ySF"
      },
      "source": [
        "The new prompt template reports some demonstration examples to instruct the model.\n",
        "\n",
        "Generally, we provide an equal number of demonstrations per class as shown in the example below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mNmRMQs4VCn"
      },
      "outputs": [],
      "source": [
        "# prompt = [\n",
        "#     {\n",
        "#         'role': 'system',\n",
        "#         'content': 'You are an annotator for sexism detection.'\n",
        "#     },\n",
        "#     {\n",
        "#         'role': 'user',\n",
        "#         'content': \"\"\"Your task is to classify input text as containing sexism or not. Respond only YES or NO.\n",
        "\n",
        "#         EXAMPLES:\n",
        "#         TEXT: **example 1**\n",
        "#         ANSWER: YES\n",
        "#         TEXT: **example 2**\n",
        "#         ANSWER: NO\n",
        "\n",
        "#         TEXT:\n",
        "#         {text}\n",
        "\n",
        "#         ANSWER:\n",
        "#         \"\"\"\n",
        "#     }\n",
        "# ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OE0XnaVr3CZz"
      },
      "source": [
        "## Instructions\n",
        "\n",
        "In order to get Task 5 points, we require you to:\n",
        "\n",
        "- Load ``demonstrations.csv`` and encode it into a ``pandas.DataFrame`` object.\n",
        "- Define a ``build_few_shot_demonstrations`` function as the one reported below.\n",
        "- Perform few-shot inference as in Task 3.\n",
        "- Compute metrics as in Task 4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "B1HfVbSxGv9_",
        "outputId": "e105897c-795a-4b88-9cba-1e2e29d9a29f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"demonstrations\",\n  \"rows\": 1000,\n  \"fields\": [\n    {\n      \"column\": \"rewire_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          \"sexism2022_english-1375\",\n          \"sexism2022_english-278\",\n          \"sexism2022_english-7964\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          \"untouched Not after 6 years, especially not if she went to college\",\n          \"I understand. We will purge hidden eurofag scum from Holy City of Miami, burn down their illegal particle accelerator labs stretching underneath the capital!\",\n          \"A Mississippi state senator hits the DC women protesters with a serious burn.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label_sexist\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"sexist\",\n          \"not sexist\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "demonstrations"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-9f49b67d-bb36-4b1b-ae9c-0dd4fba8dbf1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rewire_id</th>\n",
              "      <th>text</th>\n",
              "      <th>label_sexist</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>sexism2022_english-10348</td>\n",
              "      <td>I would never have my kid go to a daycare with...</td>\n",
              "      <td>not sexist</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>sexism2022_english-7950</td>\n",
              "      <td>Democrat women are so butt fucking ugly.</td>\n",
              "      <td>sexist</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>sexism2022_english-7742</td>\n",
              "      <td>If you really wanted to twist the knife you’d ...</td>\n",
              "      <td>not sexist</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>sexism2022_english-16170</td>\n",
              "      <td>Those women are pro-rape. They didn’t care abo...</td>\n",
              "      <td>sexist</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>sexism2022_english-9857</td>\n",
              "      <td>Married a bitch... Is it the standard scenario...</td>\n",
              "      <td>sexist</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9f49b67d-bb36-4b1b-ae9c-0dd4fba8dbf1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9f49b67d-bb36-4b1b-ae9c-0dd4fba8dbf1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9f49b67d-bb36-4b1b-ae9c-0dd4fba8dbf1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-55cd9aa7-29d9-4606-981b-6aed616fad4b\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-55cd9aa7-29d9-4606-981b-6aed616fad4b')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-55cd9aa7-29d9-4606-981b-6aed616fad4b button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                  rewire_id  \\\n",
              "0  sexism2022_english-10348   \n",
              "1   sexism2022_english-7950   \n",
              "2   sexism2022_english-7742   \n",
              "3  sexism2022_english-16170   \n",
              "4   sexism2022_english-9857   \n",
              "\n",
              "                                                text label_sexist  \n",
              "0  I would never have my kid go to a daycare with...   not sexist  \n",
              "1           Democrat women are so butt fucking ugly.       sexist  \n",
              "2  If you really wanted to twist the knife you’d ...   not sexist  \n",
              "3  Those women are pro-rape. They didn’t care abo...       sexist  \n",
              "4  Married a bitch... Is it the standard scenario...       sexist  "
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "demonstrations = pd.read_csv('./data/demonstrations.csv')\n",
        "demonstrations.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "6oyxypUG3ltH"
      },
      "outputs": [],
      "source": [
        "def build_few_shot_demonstrations(demonstrations, num_per_class=2):\n",
        "  \"\"\"\n",
        "    Inputs:\n",
        "      demonstrations: the pandas.DataFrame object wrapping demonstrations.csv\n",
        "      num_per_class: number of demonstrations per class\n",
        "\n",
        "    Outputs:\n",
        "      a list of textual demonstrations to inject into the prompt template.\n",
        "  \"\"\"\n",
        "  yes_samples = demonstrations[demonstrations['label_sexist'] == 'sexist'].sample(num_per_class)\n",
        "  no_samples = demonstrations[demonstrations['label_sexist'] == 'not sexist'].sample(num_per_class)\n",
        "\n",
        "  few_shot_demonstrations = list(pd.concat([yes_samples['text'], no_samples['text']]))\n",
        "\n",
        "  return few_shot_demonstrations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03i0vZajIKap",
        "outputId": "11f3013a-27dc-480b-b26b-b236a1470103"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Sounds like your sister is trying to make you a hostage. Get rid of that toxic feminity in your life.',\n",
              " 'Women hate vulnerable men',\n",
              " 'Do yourself a favor. Dont dignify a thing,by callin it a she.👍Thats a fuckin abomimation.',\n",
              " 'I talk to a girl then their retard friends butt in and take her away, so']"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "build_few_shot_demonstrations(demonstrations, num_per_class=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "z6ZeYYUqInlX"
      },
      "outputs": [],
      "source": [
        "def prepare_prompts_few_shot(texts, demonstrations, prompt_template, tokenizer, device):\n",
        "  \"\"\"\n",
        "    This function format input text samples into instructions prompts.\n",
        "\n",
        "    Inputs:\n",
        "      texts: input texts to classify via prompting\n",
        "      prompt_template: the prompt template provided in this assignment\n",
        "      tokenizer: the transformers Tokenizer object instance associated with the chosen model card\n",
        "\n",
        "    Outputs:\n",
        "      input texts to classify in the form of instruction prompts\n",
        "  \"\"\"\n",
        "  prompts = []\n",
        "  for text in texts:\n",
        "    prompt_with_text = deepcopy(prompt_template)\n",
        "    demonstration = ''\n",
        "    for i, d in enumerate(demonstrations):\n",
        "      if i < len(demonstrations) // 2:\n",
        "        demonstration += '\\tTEXT: ' + d + '\\n\\tANSWER: YES\\n'\n",
        "      else:\n",
        "        demonstration += '\\tTEXT: ' + d + '\\n\\tANSWER: NO\\n'\n",
        "\n",
        "    # remove extra \\t\n",
        "    demonstration = demonstration[1:]\n",
        "    prompt_with_text[1]['content'] = prompt_with_text[1]['content'].replace('{examples}', demonstration)\n",
        "\n",
        "    prompt_with_text[1]['content'] = prompt_with_text[1]['content'].replace('{text}', text)\n",
        "    prompts.append(tokenizer.apply_chat_template(prompt_with_text, tokenize=True, add_generation_prompt=True,\n",
        "              return_dict=True,\n",
        "              return_tensors=\"pt\").to(device))\n",
        "\n",
        "  return prompts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evF5otTTO-oJ"
      },
      "source": [
        "## 4-shot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "q7sb4Dd0LX7A"
      },
      "outputs": [],
      "source": [
        "prompts_few_shot = prepare_prompts_few_shot(df['text'], build_few_shot_demonstrations(demonstrations, num_per_class=2), prompt_few_shot, tokenizer, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSEpqEeXL2bW",
        "outputId": "23c028ac-1620-4c69-b076-cb563380e6d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s>[INST] You are an annotator for sexism detection.\n",
            "\n",
            "Your task is to classify input text as containing sexism or not. Respond only YES or NO.\n",
            "\n",
            "        EXAMPLES:\n",
            "        TEXT: I still can't believe they did that to David. Probably one of the most wholesome political commentators on Youtube. These days, I'm glad he's doing okay. He's recently taken to streaming on Twitch, and all the twitch thots are getting moist.\n",
            "\tANSWER: YES\n",
            "\tTEXT: yeah im gettin pussy 8) she gonna squirt, even though im a wristcels\n",
            "\tANSWER: YES\n",
            "\tTEXT: And you have saved how many from an ass kicking.\n",
            "\tANSWER: NO\n",
            "\tTEXT: [URL] Cause killing unborn babies and letting grown biological men shower with girls is upright, good & wholesome. Twisted f-ing libs.\n",
            "\tANSWER: NO\n",
            "\n",
            "\n",
            "        TEXT:\n",
            "        The boys must be gaming because there goes the wifi.\n",
            "\n",
            "        ANSWER:\n",
            "        [/INST]\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.decode(prompts_few_shot[0].input_ids[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXtyoe5KNUvE",
        "outputId": "292aceb6-7a58-4af1-d5c8-d1d502fcae51"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [06:25<00:00,  1.29s/it]\n"
          ]
        }
      ],
      "source": [
        "answers = generate_responses(model_8bit, prompts_few_shot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJs5NCE-NYj5",
        "outputId": "84e9a1aa-bbbb-4953-a294-411b65da6f27"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'accuracy': 0.6633333333333333, 'fail_ratio': 0.33666666666666667}"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "compute_metrics(answers, x_true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qw1etIg8PApy"
      },
      "source": [
        "## 8-shot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wub8v1qYO_i2",
        "outputId": "f01798d3-bd77-4d7b-a4c0-d1c2ab8cfdac"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [08:19<00:00,  1.67s/it]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'accuracy': 0.7133333333333334, 'fail_ratio': 0.2866666666666667}"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompts_few_shot = prepare_prompts_few_shot(df['text'], build_few_shot_demonstrations(demonstrations, num_per_class=4), prompt, tokenizer, device=device)\n",
        "answers = generate_responses(model_8bit, prompts_few_shot)\n",
        "compute_metrics(answers, x_true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpBFP-2XQJ4-"
      },
      "source": [
        "# Llama 3 8B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZYEEnJUQI7p",
        "outputId": "8b6d9af1-d085-490a-f79a-360e5478b701"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded on cuda:0\n"
          ]
        }
      ],
      "source": [
        "model_card = 'meta-llama/Meta-Llama-3.1-8B-Instruct'\n",
        "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_card)\n",
        "model_8bit = AutoModelForCausalLM.from_pretrained(model_card,\n",
        "                                                  quantization_config=quantization_config)\n",
        "model_8bit.eval()\n",
        "device = model_8bit.device\n",
        "print(f'Model loaded on {device}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vx0F7UJdYcjK"
      },
      "source": [
        "## Zero-shot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28x3QW0CVxKF",
        "outputId": "1161aa45-5776-4480-94f1-90688bed3c19"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/300 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/bitsandbytes/nn/modules.py:451: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
            "  warnings.warn(\n",
            "100%|██████████| 300/300 [04:49<00:00,  1.04it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'accuracy': 0.6166666666666667, 'fail_ratio': 0.38333333333333336}"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompts = prepare_prompts(df['text'], prompt, tokenizer, device=device)\n",
        "answers = generate_responses(model_8bit, prompts)\n",
        "\n",
        "x_true = df['label_sexist'] == 'sexist'\n",
        "compute_metrics(answers, x_true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSIj7LvvYfDK"
      },
      "source": [
        "## 4-shot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa5oy_O8Ybma",
        "outputId": "31185aba-ab5b-48dc-a46e-d01e45824faf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 39%|███▉      | 118/300 [03:39<05:44,  1.89s/it]"
          ]
        }
      ],
      "source": [
        "# TODO: for fair comparison, make sure that examples are the same\n",
        "prompts_few_shot = prepare_prompts_few_shot(df['text'], build_few_shot_demonstrations(demonstrations, num_per_class=2), prompt_few_shot, tokenizer, device=device)\n",
        "answers = generate_responses(model_8bit, prompts_few_shot)\n",
        "compute_metrics(answers, x_true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aN4Qd0MnZUdV"
      },
      "source": [
        "## 8-shot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQ3mRzEyZXGK"
      },
      "outputs": [],
      "source": [
        "prompts_few_shot = prepare_prompts_few_shot(df['text'], build_few_shot_demonstrations(demonstrations, num_per_class=4), prompt, tokenizer, device=device)\n",
        "answers = generate_responses(model_8bit, prompts_few_shot)\n",
        "compute_metrics(answers, x_true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGpZWQb45XXK"
      },
      "source": [
        "## Notes\n",
        "\n",
        "1. You are free to pick any value for ``num_per_class``.\n",
        "\n",
        "2. According to our tests, few-shot prompting increases inference time by some minutes (we experimented with ``num_per_class`` $\\in [2, 4]$)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHuT1a1GLJnd"
      },
      "source": [
        "# [Task 6 - 1.0 points] Error Analysis\n",
        "\n",
        "We are now interested in evaluating model responses and comparing their performance.\n",
        "\n",
        "This analysis helps us in understanding\n",
        "\n",
        "- Classification task performance gap: are the models good at this task?\n",
        "- Generation quality: which kind of responses do models generate?\n",
        "- Errors: which kind of mistakes do models do?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kjEAHD4LJne"
      },
      "source": [
        "### Instructions\n",
        "\n",
        "In order to get Task 6 points, we require you to:\n",
        "\n",
        "* Compare classification performance of selected LLMs in a Table.\n",
        "* Compute confusion matrices for selected LLMs.\n",
        "* Briefly summarize your observations on generated responses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QWlVXJgLJne"
      },
      "source": [
        "# [Task 7 - 1.0 points] Report\n",
        "\n",
        "Wrap up your experiment in a short report (up to 2 pages)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fsdV99TLJne"
      },
      "source": [
        "### Instructions\n",
        "\n",
        "* Use the NLP course report template.\n",
        "* Summarize each task in the report following the provided template."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-hUXYaLLJne"
      },
      "source": [
        "### Recommendations\n",
        "\n",
        "The report is not a copy-paste of graphs, tables, and command outputs.\n",
        "\n",
        "* Summarize classification performance in Table format.\n",
        "* **Do not** report command outputs or screenshots.\n",
        "* Report learning curves in Figure format.\n",
        "* The error analysis section should summarize your findings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fURV8zfPLJne"
      },
      "source": [
        "# Submission\n",
        "\n",
        "* **Submit** your report in PDF format.\n",
        "* **Submit** your python notebook.\n",
        "* Make sure your notebook is **well organized**, with no temporary code, commented sections, tests, etc..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zn1tUeYzLJne"
      },
      "source": [
        "# FAQ\n",
        "\n",
        "Please check this frequently asked questions before contacting us."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYAOVGvKhtTQ"
      },
      "source": [
        "### Model cards\n",
        "\n",
        "You can pick any open-source model card you like.\n",
        "\n",
        "We recommend starting from those reported in this assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWG72N-LLJne"
      },
      "source": [
        "### Implementation\n",
        "\n",
        "Everything can be done via ``transformers`` APIs.\n",
        "\n",
        "However, you are free to test frameworks, such as [LangChain](https://www.langchain.com/), [LlamaIndex](https://www.llamaindex.ai/) [LitParrot](https://github.com/awesome-software/lit-parrot), provided that you correctly address task instructions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cplnq3dLJne"
      },
      "source": [
        "### Bonus Points\n",
        "\n",
        "0.5 bonus points are arbitrarily assigned based on significant contributions such as:\n",
        "\n",
        "- Outstanding error analysis\n",
        "- Masterclass code organization\n",
        "- Suitable extensions\n",
        "- Evaluate A1 dataset and perform comparison\n",
        "\n",
        "Note that bonus points are only assigned if all task points are attributed (i.e., 6/6)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coy_pJ40LJne"
      },
      "source": [
        "### Prompt Template\n",
        "\n",
        "Do not change the provided prompt template.\n",
        "\n",
        "You are only allowed to change it in case of a possible extension."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSiH0Xqj79wc"
      },
      "source": [
        "### Optimizations\n",
        "\n",
        "Any kind of code optimization (e.g., speedup model inference or reduce computational cost) is more than welcome!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jpr-LSK7LJnh"
      },
      "source": [
        "# The End"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "celltoolbar": "Slideshow",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "01d5928c42974396ab80006ca5356e91": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e3caf2ab8e174e5a97b028e6e37c7022",
              "IPY_MODEL_68c1835888db436f926b0bf305c329d3",
              "IPY_MODEL_febe3f2209be43ff820881ac4c639286"
            ],
            "layout": "IPY_MODEL_e10b34b3ed334316ad575cf813f2ba42"
          }
        },
        "3b11ecfa0f1244e6a9285b7bcf78692c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4950ece0756e4885a3c8cd9f1bb8d606": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "595a52a1d177493291b4405059c2bba9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "68c1835888db436f926b0bf305c329d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a5cae6bf80424856ae15f4643cdf7c68",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e2a632d31e3b4c438228a3a4769f2b32",
            "value": 3
          }
        },
        "a5cae6bf80424856ae15f4643cdf7c68": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e10b34b3ed334316ad575cf813f2ba42": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e19f61f23acb42cb84b6aebb80526d7a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2a632d31e3b4c438228a3a4769f2b32": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e3caf2ab8e174e5a97b028e6e37c7022": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b11ecfa0f1244e6a9285b7bcf78692c",
            "placeholder": "​",
            "style": "IPY_MODEL_4950ece0756e4885a3c8cd9f1bb8d606",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "febe3f2209be43ff820881ac4c639286": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e19f61f23acb42cb84b6aebb80526d7a",
            "placeholder": "​",
            "style": "IPY_MODEL_595a52a1d177493291b4405059c2bba9",
            "value": " 3/3 [01:21&lt;00:00, 27.18s/it]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
